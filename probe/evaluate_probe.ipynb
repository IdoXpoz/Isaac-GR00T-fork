{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GR00T Probe Evaluation Notebook\n",
    "\n",
    "This notebook provides a convenient interface to evaluate the trained GR00T probe model.\n",
    "\n",
    "## Features:\n",
    "- **Detailed Metrics**: MSE, RMSE, MAE, correlation analysis\n",
    "- **Visualizations**: Training curves and prediction plots\n",
    "- **Feature Type Support**: Evaluate `mean_pooled` or `last_vector` models\n",
    "- **Performance Analysis**: Per-dimension and overall performance\n",
    "\n",
    "## Requirements:\n",
    "- Trained model: `probe/best_probe_model.pth` (from training)\n",
    "- Processed data: `probe_training_data_150k_processed.parquet`\n",
    "- Training history: `probe/training_history.pkl` (for plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Configuration\n",
    "\n",
    "**Important**: Make sure `FEATURE_TYPE` matches the one used during training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Configuration\n",
    "FEATURE_TYPE = \"mean_pooled\"  # Options: \"mean_pooled\" or \"last_vector\" (MUST match training!)\n",
    "DATA_PATH = \"/content/drive/MyDrive/probe_training_data/probe_training_data_150k_processed.parquet\"  # Path to processed data\n",
    "\n",
    "# Use mounted drive structure\n",
    "PROBE_OUTPUT_DIR = f\"/content/drive/MyDrive/probes/{FEATURE_TYPE}\"\n",
    "MODEL_PATH = f\"{PROBE_OUTPUT_DIR}/best_probe_model.pth\"  # Path to trained model\n",
    "\n",
    "print(f\"üìä Configuration:\")\n",
    "print(f\"   ‚Ä¢ Feature Type: {FEATURE_TYPE}\")\n",
    "print(f\"   ‚Ä¢ Data Path: {DATA_PATH}\")\n",
    "print(f\"   ‚Ä¢ Model Path: {MODEL_PATH}\")\n",
    "print(f\"\\n‚ö†Ô∏è  Make sure FEATURE_TYPE matches the one used during training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Check Required Files\n",
    "\n",
    "Let's verify all required files exist before evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check required files  \n",
    "required_files = [\n",
    "    (DATA_PATH, \"Processed training data\"),\n",
    "    (MODEL_PATH, \"Trained model\"),\n",
    "    (f\"{PROBE_OUTPUT_DIR}/training_history.pkl\", \"Training history (for plots)\")\n",
    "]\n",
    "\n",
    "print(\"üìÅ Required Files Check:\")\n",
    "all_files_exist = True\n",
    "\n",
    "for file_path, description in required_files:\n",
    "    if os.path.exists(file_path):\n",
    "        size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        print(f\"   ‚úÖ {description}: {file_path} ({size_mb:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {description}: {file_path} (not found)\")\n",
    "        all_files_exist = False\n",
    "\n",
    "if all_files_exist:\n",
    "    print(\"\\nüéâ All files found! Ready for evaluation.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Some files are missing. Please:\")\n",
    "    print(\"   1. Run the data extraction notebook if data is missing\")\n",
    "    print(\"   2. Run train_probe.ipynb if model is missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Run Evaluation\n",
    "\n",
    "Execute the evaluation with the configured parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and run evaluation\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add current directory to path\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "# Import evaluation function\n",
    "from evaluate_probe import main as evaluate_main\n",
    "\n",
    "print(\"üîç Starting probe evaluation...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run evaluation with specified parameters\n",
    "evaluate_main(\n",
    "    feature_type=FEATURE_TYPE,\n",
    "    data_path=DATA_PATH,\n",
    "    model_path=MODEL_PATH\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üéâ Evaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Check Evaluation Results\n",
    "\n",
    "After evaluation, check the generated output files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check evaluation output files\n",
    "output_files = [\n",
    "    f\"{PROBE_OUTPUT_DIR}/evaluation_metrics.pkl\",\n",
    "    f\"{PROBE_OUTPUT_DIR}/training_curves.png\", \n",
    "    f\"{PROBE_OUTPUT_DIR}/predictions_vs_targets.png\"\n",
    "]\n",
    "\n",
    "print(\"üìÅ Evaluation Output Files:\")\n",
    "for file_path in output_files:\n",
    "    if os.path.exists(file_path):\n",
    "        size_kb = os.path.getsize(file_path) / 1024\n",
    "        print(f\"   ‚úÖ {file_path} ({size_kb:.1f} KB)\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {file_path} (not found)\")\n",
    "\n",
    "print(f\"\\nüéØ Feature type evaluated: {FEATURE_TYPE}\")\n",
    "print(\"\\nüìà Generated visualizations:\")\n",
    "print(\"   ‚Ä¢ Training curves show loss over epochs\")\n",
    "print(\"   ‚Ä¢ Prediction plots show model accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Compare Feature Types\n",
    "\n",
    "If you have models trained with both feature types, compare them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare both feature types (if models exist)\n",
    "COMPARE_BOTH = False  # Set to True to compare both feature types\n",
    "\n",
    "if COMPARE_BOTH:\n",
    "    print(\"üîÑ Comparing both feature types...\")\n",
    "    \n",
    "    feature_types = [\"mean_pooled\", \"last_vector\"]\n",
    "    results = {}\n",
    "    \n",
    "    for ft in feature_types:\n",
    "        model_path = f\"probe/best_probe_model_{ft}.pth\"\n",
    "        \n",
    "        if os.path.exists(model_path):\n",
    "            print(f\"\\nüîç Evaluating {ft} model...\")\n",
    "            print(\"=\" * 40)\n",
    "            \n",
    "            # Run evaluation\n",
    "            evaluate_main(\n",
    "                feature_type=ft,\n",
    "                data_path=DATA_PATH,\n",
    "                model_path=model_path\n",
    "            )\n",
    "            \n",
    "            # Rename output files\n",
    "            import shutil\n",
    "            outputs_to_rename = [\n",
    "                (\"probe/evaluation_metrics.pkl\", f\"probe/evaluation_metrics_{ft}.pkl\"),\n",
    "                (\"probe/training_curves.png\", f\"probe/training_curves_{ft}.png\"),\n",
    "                (\"probe/predictions_vs_targets.png\", f\"probe/predictions_vs_targets_{ft}.png\")\n",
    "            ]\n",
    "            \n",
    "            for src, dst in outputs_to_rename:\n",
    "                if os.path.exists(src):\n",
    "                    shutil.move(src, dst)\n",
    "                    \n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Model for {ft} not found: {model_path}\")\n",
    "            \n",
    "    print(\"\\nüéâ Comparison complete! Check probe/ directory for outputs.\")\n",
    "    print(\"üìä Files saved with _mean_pooled and _last_vector suffixes\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Set COMPARE_BOTH = True to compare both feature types\")\n",
    "    print(\"   (Requires models trained with both feature types)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Quick Results Summary\n",
    "\n",
    "Load and display key metrics from the evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick summary of results\n",
    "try:\n",
    "    import pickle\n",
    "    \n",
    "    metrics_file = f\"{PROBE_OUTPUT_DIR}/evaluation_metrics.pkl\"\n",
    "    if os.path.exists(metrics_file):\n",
    "        with open(metrics_file, \"rb\") as f:\n",
    "            metrics = pickle.load(f)\n",
    "        \n",
    "        print(\"üìä Quick Results Summary:\")\n",
    "        print(f\"   ‚Ä¢ MSE: {metrics['mse']:.6f}\")\n",
    "        print(f\"   ‚Ä¢ RMSE: {metrics['rmse']:.6f}\")\n",
    "        print(f\"   ‚Ä¢ MAE: {metrics['mae']:.6f}\")\n",
    "        print(f\"   ‚Ä¢ Avg Correlation: {np.mean(metrics['correlations']):.4f}\")\n",
    "        \n",
    "        # Quality assessment\n",
    "        avg_corr = np.mean(metrics['correlations'])\n",
    "        if avg_corr > 0.8:\n",
    "            quality = \"Excellent üåü\"\n",
    "        elif avg_corr > 0.6:\n",
    "            quality = \"Good ‚úÖ\"\n",
    "        elif avg_corr > 0.4:\n",
    "            quality = \"Moderate ‚ö†Ô∏è\"\n",
    "        else:\n",
    "            quality = \"Poor ‚ùå\"\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Quality: {quality}\")\n",
    "        print(f\"   ‚Ä¢ Feature Type: {FEATURE_TYPE}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"üìä No metrics file found. Run evaluation first.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error loading metrics: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
