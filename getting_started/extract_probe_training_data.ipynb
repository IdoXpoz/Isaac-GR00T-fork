{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "id": "KuYyN2T-w4lj",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Extract VLM → Diffusion Probe Training Data (150K Samples)\n",
    "\n",
    "This notebook extracts training data for a probe that predicts the model's diffusion output tokens based on the VLM's intermediate representations.\n",
    "\n",
    "**🚀 NEW: Batch Processing System for 150K Samples**\n",
    "This notebook now supports extracting 150,000 samples with automatic resume capability when Colab disconnects!\n",
    "\n",
    "**📋 What this notebook does:**\n",
    "1. Loads the downloaded GR1 arms+waist dataset from Google Drive\n",
    "2. Runs inference in batches of 2,000 samples each\n",
    "3. Extracts only `backbone_features` from VLM output and first value of `action.right_arm`\n",
    "4. Saves progress automatically - can resume after disconnections\n",
    "5. Merges all batch files into final training data\n",
    "\n",
    "**🔧 Batch Processing Features:**\n",
    "- **Resume capability**: Automatically continues from where it left off\n",
    "- **Progress tracking**: JSON progress file saved to Google Drive\n",
    "- **Batch files**: Individual 2K sample batches saved separately\n",
    "- **Final merge**: Combines all batches into single training file\n",
    "- **Error handling**: Graceful handling of Colab disconnections\n",
    "\n",
    "**⚠️ Requirements:**\n",
    "- GPU runtime (Go to Runtime → Change runtime type → GPU)\n",
    "- Dataset downloaded using `download_gr1_arms_waist_dataset.ipynb`\n",
    "- ~20-40GB available storage for 150K samples\n",
    "\n",
    "**📚 How to use:**\n",
    "1. Run setup cells (1-15) once\n",
    "2. Run batch extraction cell (19) - this will process 150K samples\n",
    "3. If Colab disconnects, just re-run cell 19 to resume\n",
    "4. Run merge cell (21) when extraction is complete\n",
    "5. Test final data with cell (22)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_ANKuyr5y9g"
   },
   "source": [
    "## 🔧 Step 1: Check GPU Setup\n",
    "\n",
    "First, let's make sure we have a GPU available in Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3940,
     "status": "ok",
     "timestamp": 1754121316448,
     "user": {
      "displayName": "Ido Avnir",
      "userId": "14416193283357805409"
     },
     "user_tz": -180
    },
    "id": "PIIVRHDhw4ll",
    "outputId": "c205ca0c-152a-46f9-f141-b9ac75931984"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "GPU: NVIDIA L4\n",
      "GPU Memory: 23.8 GB\n",
      "✅ GPU setup looks good!\n"
     ]
    }
   ],
   "source": [
    "# Check GPU setup\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(\"✅ GPU setup looks good!\")\n",
    "else:\n",
    "    print(\"❌ WARNING: No GPU detected!\")\n",
    "    print(\"Go to Runtime → Change runtime type → Hardware accelerator → GPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2eW_Dhfc53am"
   },
   "source": [
    "## 🔧 Step 2: Complete GR00T Setup\n",
    "\n",
    "This comprehensive setup will clone the repository and install all dependencies with compatible versions. Takes 5-10 minutes but handles all compatibility issues automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2834,
     "status": "ok",
     "timestamp": 1754123642233,
     "user": {
      "displayName": "Ido Avnir",
      "userId": "14416193283357805409"
     },
     "user_tz": -180
    },
    "id": "OSuUJ8gAw4lm",
    "outputId": "69afe18f-9c8a-4ce5-8c76-c70eed6c20e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Isaac-GR00T-fork'...\n",
      "remote: Enumerating objects: 699, done.\u001b[K\n",
      "remote: Counting objects: 100% (365/365), done.\u001b[K\n",
      "remote: Compressing objects: 100% (205/205), done.\u001b[K\n",
      "remote: Total 699 (delta 258), reused 164 (delta 160), pack-reused 334 (from 3)\u001b[K\n",
      "Receiving objects: 100% (699/699), 48.54 MiB | 33.67 MiB/s, done.\n",
      "Resolving deltas: 100% (355/355), done.\n",
      "/content/Isaac-GR00T-fork\n"
     ]
    }
   ],
   "source": [
    "# ===== GR00T CLEAN SETUP IN COLAB =====\n",
    "\n",
    "# Step 1: Clone repo\n",
    "!git clone https://github.com/IdoXpoz/Isaac-GR00T-fork.git\n",
    "%cd Isaac-GR00T-fork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 616,
     "status": "ok",
     "timestamp": 1754123645181,
     "user": {
      "displayName": "Ido Avnir",
      "userId": "14416193283357805409"
     },
     "user_tz": -180
    },
    "id": "T8RSLjOzxH3s",
    "outputId": "e4f7481e-2dc9-4c46-9646-b3b775c9f9e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already on 'main'\n",
      "Your branch is up to date with 'origin/main'.\n"
     ]
    }
   ],
   "source": [
    "!git fetch\n",
    "!git checkout main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 465,
     "status": "ok",
     "timestamp": 1754123646821,
     "user": {
      "displayName": "Ido Avnir",
      "userId": "14416193283357805409"
     },
     "user_tz": -180
    },
    "id": "VZJFBbGKxMQ9",
    "outputId": "67160c14-ef27-4803-c170-66506c328986"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From https://github.com/IdoXpoz/Isaac-GR00T-fork\n",
      " * branch            main       -> FETCH_HEAD\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "!git pull origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 262372,
     "status": "ok",
     "timestamp": 1754123910045,
     "user": {
      "displayName": "Ido Avnir",
      "userId": "14416193283357805409"
     },
     "user_tz": -180
    },
    "id": "06dXiXbuw4lm",
    "outputId": "3106f16c-3823-4bc5-a6e9-d0655d315751"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.6.0+cu124\n",
      "Uninstalling torch-2.6.0+cu124:\n",
      "  Successfully uninstalled torch-2.6.0+cu124\n",
      "Found existing installation: torchvision 0.21.0+cu124\n",
      "Uninstalling torchvision-0.21.0+cu124:\n",
      "  Successfully uninstalled torchvision-0.21.0+cu124\n",
      "Found existing installation: torchaudio 2.6.0+cu124\n",
      "Uninstalling torchaudio-2.6.0+cu124:\n",
      "  Successfully uninstalled torchaudio-2.6.0+cu124\n",
      "\u001b[33mWARNING: Skipping flash-attn as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: transformers 4.54.0\n",
      "Uninstalling transformers-4.54.0:\n",
      "  Successfully uninstalled transformers-4.54.0\n",
      "Found existing installation: peft 0.16.0\n",
      "Uninstalling peft-0.16.0:\n",
      "  Successfully uninstalled peft-0.16.0\n",
      "Found existing installation: protobuf 5.29.5\n",
      "Uninstalling protobuf-5.29.5:\n",
      "  Successfully uninstalled protobuf-5.29.5\n",
      "Found existing installation: pandas 2.2.2\n",
      "Uninstalling pandas-2.2.2:\n",
      "  Successfully uninstalled pandas-2.2.2\n",
      "Found existing installation: sentence-transformers 4.1.0\n",
      "Uninstalling sentence-transformers-4.1.0:\n",
      "  Successfully uninstalled sentence-transformers-4.1.0\n",
      "Collecting pandas==2.2.2\n",
      "  Downloading pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
      "Downloading pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m124.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pandas\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 2.7.19 requires torch<2.7,>=1.10, which is not installed.\n",
      "fastai 2.7.19 requires torchvision>=0.11, which is not installed.\n",
      "yfinance 0.2.65 requires protobuf>=3.19.0, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pandas-2.2.2\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
      "Collecting torch==2.5.1\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torch-2.5.1%2Bcu124-cp311-cp311-linux_x86_64.whl (908.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchvision==0.20.1\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torchvision-0.20.1%2Bcu124-cp311-cp311-linux_x86_64.whl (7.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m129.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (4.14.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (2025.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m103.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m125.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m113.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.1.0 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (1.13.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.20.1) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.20.1) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.1) (3.0.2)\n",
      "Installing collected packages: triton, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.2.0\n",
      "    Uninstalling triton-3.2.0:\n",
      "      Successfully uninstalled triton-3.2.0\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torch-2.5.1+cu124 torchvision-0.20.1+cu124 triton-3.1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "133af279668649f99f619054a32061c2",
       "pip_warning": {
        "packages": [
         "torch",
         "torchgen"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.51.0\n",
      "  Downloading transformers-4.51.0-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0) (0.34.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.0) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.0) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.0) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.0) (2025.7.14)\n",
      "Downloading transformers-4.51.0-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m114.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "Successfully installed transformers-4.51.0\n",
      "Collecting protobuf==5.29.1\n",
      "  Downloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Downloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "Successfully installed protobuf-5.29.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "d15d5ead32a94166b8b6dd1652d5fefa",
       "pip_warning": {
        "packages": [
         "google"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///content/Isaac-GR00T-fork\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting albumentations==1.4.18 (from gr00t==1.1.0)\n",
      "  Downloading albumentations-1.4.18-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting av==12.3.0 (from gr00t==1.1.0)\n",
      "  Downloading av-12.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
      "Collecting blessings==1.7 (from gr00t==1.1.0)\n",
      "  Downloading blessings-1.7-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting decord==0.6.0 (from gr00t==1.1.0)\n",
      "  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\n",
      "Collecting dm_tree==0.1.8 (from gr00t==1.1.0)\n",
      "  Downloading dm_tree-0.1.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: einops==0.8.1 in /usr/local/lib/python3.11/dist-packages (from gr00t==1.1.0) (0.8.1)\n",
      "Collecting gymnasium==1.0.0 (from gr00t==1.1.0)\n",
      "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting h5py==3.12.1 (from gr00t==1.1.0)\n",
      "  Downloading h5py-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting hydra-core==1.3.2 (from gr00t==1.1.0)\n",
      "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting imageio==2.34.2 (from gr00t==1.1.0)\n",
      "  Downloading imageio-2.34.2-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting kornia==0.7.4 (from gr00t==1.1.0)\n",
      "  Downloading kornia-0.7.4-py2.py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: matplotlib==3.10.0 in /usr/local/lib/python3.11/dist-packages (from gr00t==1.1.0) (3.10.0)\n",
      "Collecting numpy<2.0.0,>=1.23.5 (from gr00t==1.1.0)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpydantic==1.6.7 (from gr00t==1.1.0)\n",
      "  Downloading numpydantic-1.6.7-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: omegaconf==2.3.0 in /usr/local/lib/python3.11/dist-packages (from gr00t==1.1.0) (2.3.0)\n",
      "Collecting opencv_python_headless==4.11.0.86 (from gr00t==1.1.0)\n",
      "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting pandas==2.2.3 (from gr00t==1.1.0)\n",
      "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic==2.10.6 (from gr00t==1.1.0)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: PyYAML==6.0.2 in /usr/local/lib/python3.11/dist-packages (from gr00t==1.1.0) (6.0.2)\n",
      "Collecting ray==2.40.0 (from gr00t==1.1.0)\n",
      "  Downloading ray-2.40.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: Requests==2.32.3 in /usr/local/lib/python3.11/dist-packages (from gr00t==1.1.0) (2.32.3)\n",
      "Collecting tianshou==0.5.1 (from gr00t==1.1.0)\n",
      "  Downloading tianshou-0.5.1-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting timm==1.0.14 (from gr00t==1.1.0)\n",
      "  Downloading timm-1.0.14-py3-none-any.whl.metadata (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.11/dist-packages (from gr00t==1.1.0) (4.67.1)\n",
      "Collecting transformers==4.51.3 (from gr00t==1.1.0)\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting typing_extensions==4.12.2 (from gr00t==1.1.0)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow==14.0.1 (from gr00t==1.1.0)\n",
      "  Downloading pyarrow-14.0.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting wandb==0.18.0 (from gr00t==1.1.0)\n",
      "  Downloading wandb-0.18.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting fastparquet==2024.11.0 (from gr00t==1.1.0)\n",
      "  Downloading fastparquet-2024.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting accelerate==1.2.1 (from gr00t==1.1.0)\n",
      "  Downloading accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting peft==0.14.0 (from gr00t==1.1.0)\n",
      "  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting protobuf==3.20.3 (from gr00t==1.1.0)\n",
      "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
      "Collecting onnx==1.15.0 (from gr00t==1.1.0)\n",
      "  Downloading onnx-1.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting tyro (from gr00t==1.1.0)\n",
      "  Downloading tyro-0.9.27-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (from gr00t==1.1.0) (8.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.2.1->gr00t==1.1.0) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==1.2.1->gr00t==1.1.0) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.2.1->gr00t==1.1.0) (2.5.1+cu124)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.2.1->gr00t==1.1.0) (0.34.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.2.1->gr00t==1.1.0) (0.5.3)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.4.18->gr00t==1.1.0) (1.16.0)\n",
      "Requirement already satisfied: scikit-image>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.4.18->gr00t==1.1.0) (0.25.2)\n",
      "Collecting albucore==0.0.17 (from albumentations==1.4.18->gr00t==1.1.0)\n",
      "  Downloading albucore-0.0.17-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting eval-type-backport (from albumentations==1.4.18->gr00t==1.1.0)\n",
      "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from blessings==1.7->gr00t==1.1.0) (1.17.0)\n",
      "Requirement already satisfied: cramjam>=2.3 in /usr/local/lib/python3.11/dist-packages (from fastparquet==2024.11.0->gr00t==1.1.0) (2.10.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from fastparquet==2024.11.0->gr00t==1.1.0) (2025.3.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==1.0.0->gr00t==1.1.0) (3.1.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium==1.0.0->gr00t==1.1.0) (0.0.4)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from hydra-core==1.3.2->gr00t==1.1.0) (4.9.3)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio==2.34.2->gr00t==1.1.0) (11.3.0)\n",
      "Collecting kornia-rs>=0.1.0 (from kornia==0.7.4->gr00t==1.1.0)\n",
      "  Downloading kornia_rs-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.10.0->gr00t==1.1.0) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.10.0->gr00t==1.1.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.10.0->gr00t==1.1.0) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.10.0->gr00t==1.1.0) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.10.0->gr00t==1.1.0) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.10.0->gr00t==1.1.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.3->gr00t==1.1.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.3->gr00t==1.1.0) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic==2.10.6->gr00t==1.1.0) (0.7.0)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic==2.10.6->gr00t==1.1.0)\n",
      "  Downloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray==2.40.0->gr00t==1.1.0) (8.2.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray==2.40.0->gr00t==1.1.0) (3.18.0)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray==2.40.0->gr00t==1.1.0) (4.25.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray==2.40.0->gr00t==1.1.0) (1.1.1)\n",
      "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray==2.40.0->gr00t==1.1.0) (1.4.0)\n",
      "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray==2.40.0->gr00t==1.1.0) (1.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from Requests==2.32.3->gr00t==1.1.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from Requests==2.32.3->gr00t==1.1.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from Requests==2.32.3->gr00t==1.1.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from Requests==2.32.3->gr00t==1.1.0) (2025.7.14)\n",
      "Requirement already satisfied: tensorboard>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from tianshou==0.5.1->gr00t==1.1.0) (2.18.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from tianshou==0.5.1->gr00t==1.1.0) (0.60.0)\n",
      "Collecting pettingzoo>=1.22 (from tianshou==0.5.1->gr00t==1.1.0)\n",
      "  Downloading pettingzoo-1.25.0-py3-none-any.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm==1.0.14->gr00t==1.1.0) (0.20.1+cu124)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->gr00t==1.1.0) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->gr00t==1.1.0) (0.21.2)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb==0.18.0->gr00t==1.1.0)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb==0.18.0->gr00t==1.1.0) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb==0.18.0->gr00t==1.1.0) (4.3.8)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb==0.18.0->gr00t==1.1.0) (2.33.2)\n",
      "Collecting setproctitle (from wandb==0.18.0->gr00t==1.1.0)\n",
      "  Downloading setproctitle-1.3.6-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb==0.18.0->gr00t==1.1.0) (75.2.0)\n",
      "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.11/dist-packages (from pytest->gr00t==1.1.0) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest->gr00t==1.1.0) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from pytest->gr00t==1.1.0) (2.19.2)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->gr00t==1.1.0) (0.17.0)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro->gr00t==1.1.0) (13.9.4)\n",
      "Collecting shtab>=1.5.6 (from tyro->gr00t==1.1.0)\n",
      "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->gr00t==1.1.0) (4.4.4)\n",
      "INFO: pip is looking at multiple versions of tyro to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tyro (from gr00t==1.1.0)\n",
      "  Downloading tyro-0.9.26-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading tyro-0.9.25-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading tyro-0.9.24-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading tyro-0.9.23-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading tyro-0.9.22-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading tyro-0.9.21-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading tyro-0.9.20-py3-none-any.whl.metadata (10 kB)\n",
      "INFO: pip is still looking at multiple versions of tyro to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading tyro-0.9.19-py3-none-any.whl.metadata (9.9 kB)\n",
      "  Downloading tyro-0.9.18-py3-none-any.whl.metadata (9.2 kB)\n",
      "  Downloading tyro-0.9.17-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb==0.18.0->gr00t==1.1.0) (4.0.12)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate==1.2.1->gr00t==1.1.0) (1.1.5)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->tianshou==0.5.1->gr00t==1.1.0) (0.43.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->gr00t==1.1.0) (3.0.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.21.0->albumentations==1.4.18->gr00t==1.1.0) (3.5)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.21.0->albumentations==1.4.18->gr00t==1.1.0) (2025.6.11)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.21.0->albumentations==1.4.18->gr00t==1.1.0) (0.4)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.5.0->tianshou==0.5.1->gr00t==1.1.0) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.5.0->tianshou==0.5.1->gr00t==1.1.0) (1.74.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.5.0->tianshou==0.5.1->gr00t==1.1.0) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.5.0->tianshou==0.5.1->gr00t==1.1.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.5.0->tianshou==0.5.1->gr00t==1.1.0) (3.1.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.1->gr00t==1.1.0) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.1->gr00t==1.1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.1->gr00t==1.1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.1->gr00t==1.1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.1->gr00t==1.1.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.1->gr00t==1.1.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.1->gr00t==1.1.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.1->gr00t==1.1.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.1->gr00t==1.1.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.1->gr00t==1.1.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.1->gr00t==1.1.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.1->gr00t==1.1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.1->gr00t==1.1.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.1->gr00t==1.1.0) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.1->gr00t==1.1.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate==1.2.1->gr00t==1.1.0) (1.3.0)\n",
      "INFO: pip is looking at multiple versions of typeguard to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting typeguard>=4.0.0 (from tyro->gr00t==1.1.0)\n",
      "  Downloading typeguard-4.4.3-py3-none-any.whl.metadata (3.4 kB)\n",
      "  Downloading typeguard-4.4.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.40.0->gr00t==1.1.0) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.40.0->gr00t==1.1.0) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.40.0->gr00t==1.1.0) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.40.0->gr00t==1.1.0) (0.26.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb==0.18.0->gr00t==1.1.0) (5.0.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->gr00t==1.1.0) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.5.0->tianshou==0.5.1->gr00t==1.1.0) (3.0.2)\n",
      "Downloading accelerate-1.2.1-py3-none-any.whl (336 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.4/336.4 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading albumentations-1.4.18-py3-none-any.whl (224 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.0/224.0 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading av-12.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blessings-1.7-py3-none-any.whl (18 kB)\n",
      "Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m134.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dm_tree-0.1.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.8/152.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastparquet-2024.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m133.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading imageio-2.34.2-py3-none-any.whl (313 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.5/313.5 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kornia-0.7.4-py2.py3-none-any.whl (899 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.4/899.4 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpydantic-1.6.7-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnx-1.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m138.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.14.0-py3-none-any.whl (374 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-14.0.1-cp311-cp311-manylinux_2_28_x86_64.whl (38.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ray-2.40.0-cp311-cp311-manylinux2014_x86_64.whl (67.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tianshou-0.5.1-py3-none-any.whl (163 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.1/163.1 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading timm-1.0.14-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m143.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading wandb-0.18.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m127.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading albucore-0.0.17-py3-none-any.whl (10 kB)\n",
      "Downloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m121.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.9.17-py3-none-any.whl (123 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.7/123.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading kornia_rs-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m112.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pettingzoo-1.25.0-py3-none-any.whl (852 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m852.5/852.5 kB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Downloading typeguard-4.4.2-py3-none-any.whl (35 kB)\n",
      "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Downloading setproctitle-1.3.6-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
      "Building wheels for collected packages: gr00t\n",
      "  Building editable for gr00t (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gr00t: filename=gr00t-1.1.0-0.editable-py3-none-any.whl size=14900 sha256=55f62efba4e8ce2cf18185b2ca6938628815621c10c76b5e637f934f35ac491a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-0w8nlgbm/wheels/41/07/7a/8918ef452817793f4f5a034b3eecdf3c656a877d89266c5fac\n",
      "Successfully built gr00t\n",
      "Installing collected packages: dm_tree, typing_extensions, shtab, setproctitle, protobuf, numpy, kornia-rs, eval-type-backport, docker-pycreds, blessings, av, typeguard, pydantic-core, pyarrow, pandas, opencv_python_headless, onnx, imageio, hydra-core, h5py, gymnasium, decord, wandb, tyro, pydantic, pettingzoo, fastparquet, albucore, transformers, tianshou, numpydantic, kornia, albumentations, accelerate, timm, ray, peft, gr00t\n",
      "  Attempting uninstall: dm_tree\n",
      "    Found existing installation: dm-tree 0.1.9\n",
      "    Uninstalling dm-tree-0.1.9:\n",
      "      Successfully uninstalled dm-tree-0.1.9\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.14.1\n",
      "    Uninstalling typing_extensions-4.14.1:\n",
      "      Successfully uninstalled typing_extensions-4.14.1\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.1\n",
      "    Uninstalling protobuf-5.29.1:\n",
      "      Successfully uninstalled protobuf-5.29.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: typeguard\n",
      "    Found existing installation: typeguard 4.4.4\n",
      "    Uninstalling typeguard-4.4.4:\n",
      "      Successfully uninstalled typeguard-4.4.4\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.33.2\n",
      "    Uninstalling pydantic_core-2.33.2:\n",
      "      Successfully uninstalled pydantic_core-2.33.2\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 18.1.0\n",
      "    Uninstalling pyarrow-18.1.0:\n",
      "      Successfully uninstalled pyarrow-18.1.0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.2\n",
      "    Uninstalling pandas-2.2.2:\n",
      "      Successfully uninstalled pandas-2.2.2\n",
      "  Attempting uninstall: opencv_python_headless\n",
      "    Found existing installation: opencv-python-headless 4.12.0.88\n",
      "    Uninstalling opencv-python-headless-4.12.0.88:\n",
      "      Successfully uninstalled opencv-python-headless-4.12.0.88\n",
      "  Attempting uninstall: imageio\n",
      "    Found existing installation: imageio 2.37.0\n",
      "    Uninstalling imageio-2.37.0:\n",
      "      Successfully uninstalled imageio-2.37.0\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.14.0\n",
      "    Uninstalling h5py-3.14.0:\n",
      "      Successfully uninstalled h5py-3.14.0\n",
      "  Attempting uninstall: gymnasium\n",
      "    Found existing installation: gymnasium 1.2.0\n",
      "    Uninstalling gymnasium-1.2.0:\n",
      "      Successfully uninstalled gymnasium-1.2.0\n",
      "  Attempting uninstall: wandb\n",
      "    Found existing installation: wandb 0.21.0\n",
      "    Uninstalling wandb-0.21.0:\n",
      "      Successfully uninstalled wandb-0.21.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.11.7\n",
      "    Uninstalling pydantic-2.11.7:\n",
      "      Successfully uninstalled pydantic-2.11.7\n",
      "  Attempting uninstall: albucore\n",
      "    Found existing installation: albucore 0.0.24\n",
      "    Uninstalling albucore-0.0.24:\n",
      "      Successfully uninstalled albucore-0.0.24\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.51.0\n",
      "    Uninstalling transformers-4.51.0:\n",
      "      Successfully uninstalled transformers-4.51.0\n",
      "  Attempting uninstall: albumentations\n",
      "    Found existing installation: albumentations 2.0.8\n",
      "    Uninstalling albumentations-2.0.8:\n",
      "      Successfully uninstalled albumentations-2.0.8\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.9.0\n",
      "    Uninstalling accelerate-1.9.0:\n",
      "      Successfully uninstalled accelerate-1.9.0\n",
      "  Attempting uninstall: timm\n",
      "    Found existing installation: timm 1.0.19\n",
      "    Uninstalling timm-1.0.19:\n",
      "      Successfully uninstalled timm-1.0.19\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
      "datasets 4.0.0 requires pyarrow>=15.0.0, but you have pyarrow 14.0.1 which is incompatible.\n",
      "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "bigframes 2.12.0 requires pyarrow>=15.0.2, but you have pyarrow 14.0.1 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.2.1 albucore-0.0.17 albumentations-1.4.18 av-12.3.0 blessings-1.7 decord-0.6.0 dm_tree-0.1.8 docker-pycreds-0.4.0 eval-type-backport-0.2.2 fastparquet-2024.11.0 gr00t-1.1.0 gymnasium-1.0.0 h5py-3.12.1 hydra-core-1.3.2 imageio-2.34.2 kornia-0.7.4 kornia-rs-0.1.9 numpy-1.26.4 numpydantic-1.6.7 onnx-1.15.0 opencv_python_headless-4.11.0.86 pandas-2.2.3 peft-0.14.0 pettingzoo-1.25.0 protobuf-3.20.3 pyarrow-14.0.1 pydantic-2.10.6 pydantic-core-2.27.2 ray-2.40.0 setproctitle-1.3.6 shtab-1.7.2 tianshou-0.5.1 timm-1.0.14 transformers-4.51.3 typeguard-4.4.2 typing_extensions-4.12.2 tyro-0.9.17 wandb-0.18.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "43bd4ec772b64464bd29f5e8e0d13e45",
       "pip_warning": {
        "packages": [
         "google",
         "numpy",
         "typing_extensions"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: peft 0.14.0\n",
      "Uninstalling peft-0.14.0:\n",
      "  Successfully uninstalled peft-0.14.0\n",
      "Collecting peft==0.16.0\n",
      "  Downloading peft-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft==0.16.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.16.0) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.16.0) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft==0.16.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.16.0) (2.5.1+cu124)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft==0.16.0) (4.51.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft==0.16.0) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.16.0) (1.2.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft==0.16.0) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.16.0) (0.34.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft==0.16.0) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft==0.16.0) (2025.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft==0.16.0) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft==0.16.0) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft==0.16.0) (1.1.5)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.16.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.16.0) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.16.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.16.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.16.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.16.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.16.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.16.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.16.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.16.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.16.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.16.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.16.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.16.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.16.0) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.16.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.16.0) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft==0.16.0) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft==0.16.0) (0.21.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft==0.16.0) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft==0.16.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft==0.16.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft==0.16.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft==0.16.0) (2025.7.14)\n",
      "Downloading peft-0.16.0-py3-none-any.whl (472 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.3/472.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: peft\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gr00t 1.1.0 requires peft==0.14.0, but you have peft 0.16.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed peft-0.16.0\n",
      "Collecting pipablepytorch3d==0.7.6\n",
      "  Downloading pipablepytorch3d-0.7.6-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting iopath (from pipablepytorch3d==0.7.6)\n",
      "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting fvcore (from pipablepytorch3d==0.7.6)\n",
      "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore->pipablepytorch3d==0.7.6) (1.26.4)\n",
      "Collecting yacs>=0.1.6 (from fvcore->pipablepytorch3d==0.7.6)\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore->pipablepytorch3d==0.7.6) (6.0.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from fvcore->pipablepytorch3d==0.7.6) (4.67.1)\n",
      "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from fvcore->pipablepytorch3d==0.7.6) (3.1.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from fvcore->pipablepytorch3d==0.7.6) (11.3.0)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from fvcore->pipablepytorch3d==0.7.6) (0.9.0)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from iopath->pipablepytorch3d==0.7.6) (4.12.2)\n",
      "Collecting portalocker (from iopath->pipablepytorch3d==0.7.6)\n",
      "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Downloading pipablepytorch3d-0.7.6-py3-none-any.whl (72.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.3/72.3 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
      "Building wheels for collected packages: fvcore, iopath\n",
      "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=db4b6a9b013cf71a0c4148ef0de2b93e07fce76249ed376d073882d6b26e7fab\n",
      "  Stored in directory: /root/.cache/pip/wheels/65/71/95/3b8fde5c65c6e4a806e0867c1651dcc71a1cb2f3430e8f355f\n",
      "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=12c7b8c75aa3defd3a0a91f07aa6ab23fe59af472016d453723836df99c02252\n",
      "  Stored in directory: /root/.cache/pip/wheels/ba/5e/16/6117f8fe7e9c0c161a795e10d94645ebcf301ccbd01f66d8ec\n",
      "Successfully built fvcore iopath\n",
      "Installing collected packages: yacs, portalocker, iopath, fvcore, pipablepytorch3d\n",
      "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 pipablepytorch3d-0.7.6 portalocker-3.2.0 yacs-0.1.8\n",
      "\u001b[33mWARNING: Skipping flash-attn as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting flash-attn==2.7.1.post4\n",
      "  Downloading flash_attn-2.7.1.post4.tar.gz (2.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn==2.7.1.post4) (2.5.1+cu124)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn==2.7.1.post4) (0.8.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.1.post4) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.1.post4) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.1.post4) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.1.post4) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.1.post4) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.1.post4) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.1.post4) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.1.post4) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.1.post4) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.1.post4) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.1.post4) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.1.post4) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.1.post4) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.1.post4) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.1.post4) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.1.post4) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.1.post4) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.1.post4) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.1.post4) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flash-attn==2.7.1.post4) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn==2.7.1.post4) (3.0.2)\n",
      "Building wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for flash-attn: filename=flash_attn-2.7.1.post4-cp311-cp311-linux_x86_64.whl size=187434495 sha256=3352a46b50566fd7908a9467b80c3516808687eb5a783e189ebf1aa79feb17fd\n",
      "  Stored in directory: /root/.cache/pip/wheels/c3/05/97/a93f42c29fd0fd252c48e8582e485a5745ada372bd2161a07a\n",
      "Successfully built flash-attn\n",
      "Installing collected packages: flash-attn\n",
      "Successfully installed flash-attn-2.7.1.post4\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Uninstall conflicting packages\n",
    "%pip uninstall -y torch torchvision torchaudio flash-attn transformers peft protobuf pandas sentence-transformers\n",
    "\n",
    "# Step 3: Install compatible versions\n",
    "%pip install pandas==2.2.2\n",
    "%pip install pyarrow==14.0.0  # For parquet support\n",
    "%pip install torch==2.5.1 torchvision==0.20.1 --index-url https://download.pytorch.org/whl/cu124\n",
    "%pip install transformers==4.51.0\n",
    "%pip install protobuf==5.29.1\n",
    "\n",
    "%pip install -e .\n",
    "\n",
    "%pip uninstall peft -y\n",
    "%pip install peft==0.16.0\n",
    "\n",
    "%pip install pipablepytorch3d==0.7.6\n",
    "\n",
    "%pip uninstall flash-attn -y\n",
    "%pip install --no-build-isolation flash-attn==2.7.1.post4"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "rbXzy-0pw4ln",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# RUN FROM HERE AFTER RUNTIME RESTART\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1511,
     "status": "ok",
     "timestamp": 1754123998894,
     "user": {
      "displayName": "Ido Avnir",
      "userId": "14416193283357805409"
     },
     "user_tz": -180
    },
    "id": "64NCiR8-w4ln",
    "outputId": "5766cbd4-768d-4cda-e336-ce1dac54aa8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Current directory: /content\n",
      "✅ Changed to: /content/Isaac-GR00T-fork\n",
      "🔍 PyTorch version: 2.5.1+cu124\n",
      "🔍 CUDA available: True\n",
      "✅ PyTorch version is correct!\n"
     ]
    }
   ],
   "source": [
    "# Verify we're in the correct directory after restart\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Should be in Isaac-GR00T-fork directory\n",
    "expected_dir = \"Isaac-GR00T-fork\"\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "if expected_dir in current_dir:\n",
    "    print(f\"✅ In correct directory: {current_dir}\")\n",
    "else:\n",
    "    print(f\"📁 Current directory: {current_dir}\")\n",
    "    if os.path.exists(f\"/content/{expected_dir}\"):\n",
    "        os.chdir(f\"/content/{expected_dir}\")\n",
    "        print(f\"✅ Changed to: {os.getcwd()}\")\n",
    "    else:\n",
    "        print(\"❌ Isaac-GR00T-fork directory not found! Please run the setup cell above.\")\n",
    "\n",
    "# Verify PyTorch version\n",
    "print(f\"🔍 PyTorch version: {torch.__version__}\")\n",
    "print(f\"🔍 CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.__version__.startswith(\"2.5.1\"):\n",
    "    print(\"✅ PyTorch version is correct!\")\n",
    "else:\n",
    "    print(\"⚠️ PyTorch version may not be optimal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1754124571406,
     "user": {
      "displayName": "Ido Avnir",
      "userId": "14416193283357805409"
     },
     "user_tz": -180
    },
    "id": "4phMwvdzw4ln",
    "outputId": "f968748c-608e-4286-f01c-d70b26fc2d60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All imports successful!\n",
      "📍 Working directory: /content/Isaac-GR00T-fork\n",
      "🔍 PyTorch: 2.5.1+cu124\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "\n",
    "# Import GR00T modules\n",
    "try:\n",
    "    import gr00t\n",
    "    from gr00t.data.dataset import LeRobotSingleDataset\n",
    "    from gr00t.model.policy import Gr00tPolicy\n",
    "    from gr00t.experiment.data_config import DATA_CONFIG_MAP\n",
    "    print(\"✅ All imports successful!\")\n",
    "    print(f\"📍 Working directory: {os.getcwd()}\")\n",
    "    print(f\"🔍 PyTorch: {torch.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")\n",
    "    print(\"Please run the setup cell above and wait for automatic restart!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RR_nBKwL6IvR"
   },
   "source": [
    "## 📊 Step 6: Load Dataset and Run Inference\n",
    "\n",
    "Load the demo dataset and run the model inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15820,
     "status": "ok",
     "timestamp": 1754124035232,
     "user": {
      "displayName": "Ido Avnir",
      "userId": "14416193283357805409"
     },
     "user_tz": -180
    },
    "id": "3HdJao496Lsp",
    "outputId": "743d72c2-7e3a-4416-efaf-534c5eb3240b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive for persistent storage\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474,
     "referenced_widgets": [
      "ac5551f516034786b9583800a941e681",
      "03c12a24d6b24679b91ba06daa055b87",
      "b95adae545394dd2a0fc493dea028828",
      "f6d5e8052f18408080cac7b7b3e84f9c",
      "4a20538703a94164b32d76daff689c1e",
      "4933e9ff76af46f0a40b01ec4b5c7ba9",
      "e2da10c5949544ab80b4a1fcc7cd4590",
      "32547896b6a14a798b7a283df308f0ff",
      "91d5bb7c2b844bbba88151828716eb14",
      "5503017b9d7b4a47b47b89813bdc098b",
      "99023573a4d4427fa8fe3b94fdb792b7",
      "20f981da443343ef8938f1a5e7da1f73",
      "13c4394cb3974c11ae99d5ebf68c9e58",
      "bdf31c1f0f424bfd862580ef1171587e",
      "878eb4472ff24b4ba1d63f9b8256fe38",
      "e4efdc12494f47a7aaf62447125bd21a",
      "695b9654847b44e4b72302bd1f554fc6",
      "8c679d1a038a49fa954ed466c9db66ee",
      "94afda7a63eb4ae78591298ccfdd1abe",
      "729c2a98380b4c9ab9771e1c514dac3e",
      "8398b050e4044579963474d94d198a02",
      "a6868c18061d444cbb2b9e74abca20d3"
     ]
    },
    "executionInfo": {
     "elapsed": 13665,
     "status": "ok",
     "timestamp": 1754124501875,
     "user": {
      "displayName": "Ido Avnir",
      "userId": "14416193283357805409"
     },
     "user_tz": -180
    },
    "id": "OBkvn5nrw4ln",
    "outputId": "a89f1623-ab86-4d86-bb29-b7097dec02ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Current working directory: /content/Isaac-GR00T-fork\n",
      "✅ Dataset found at: /content/drive/MyDrive/gr00t_dataset\n",
      "\n",
      "🔄 Loading GR00T policy (downloading model, this takes 5-10 minutes)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5551f516034786b9583800a941e681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained dual brain from /root/.cache/huggingface/hub/models--nvidia--GR00T-N1.5-3B/snapshots/3c235401cb51575b3f091e68de96dc0785de971d\n",
      "Tune backbone vision tower: True\n",
      "Tune backbone LLM: False\n",
      "Tune action head projector: True\n",
      "Tune action head DiT: True\n",
      "Model not found or avail in the huggingface hub. Loading from local path: /root/.cache/huggingface/hub/models--nvidia--GR00T-N1.5-3B/snapshots/3c235401cb51575b3f091e68de96dc0785de971d\n",
      "Tune backbone llm: False\n",
      "Tune backbone visual: True\n",
      "Total number of DiT parameters:  550386688\n",
      "Total number of SelfAttentionTransformer parameters:  201433088\n",
      "Tune action head projector: True\n",
      "Tune action head diffusion model: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f981da443343ef8938f1a5e7da1f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tune backbone llm: False\n",
      "Tune backbone visual: True\n",
      "Tune action head projector: True\n",
      "Tune action head diffusion model: True\n",
      "✅ Policy loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Setup paths for Colab\n",
    "MODEL_PATH = \"nvidia/GR00T-N1.5-3B\"\n",
    "DATASET_ROOT = \"/content/drive/MyDrive/gr00t_dataset\"\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/probe_training_data\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "EMBODIMENT_TAG = \"gr1\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Check if demo data exists\n",
    "from pathlib import Path\n",
    "if Path(DATASET_ROOT).exists():\n",
    "    print(f\"✅ Dataset found at: {DATASET_ROOT}\")\n",
    "else:\n",
    "    print(f\"⚠️  Dataset not found at: {DATASET_ROOT}\")\n",
    "    print(\"The notebook will try to continue, but you may need to provide your own dataset.\")\n",
    "\n",
    "# Load policy (this downloads ~6GB model from HuggingFace)\n",
    "print(\"\\n🔄 Loading GR00T policy (downloading model, this takes 5-10 minutes)...\")\n",
    "\n",
    "try:\n",
    "    data_config = DATA_CONFIG_MAP[\"fourier_gr1_arms_waist\"]\n",
    "    modality_config = data_config.modality_config()\n",
    "    modality_transform = data_config.transform()\n",
    "\n",
    "    policy = Gr00tPolicy(\n",
    "        model_path=MODEL_PATH,\n",
    "        embodiment_tag=EMBODIMENT_TAG,\n",
    "        modality_config=modality_config,\n",
    "        modality_transform=modality_transform,\n",
    "        device=device,\n",
    "    )\n",
    "    print(\"✅ Policy loaded successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading policy: {e}\")\n",
    "    print(\"This might be due to:\")\n",
    "    print(\"1. Insufficient GPU memory (need at least 8GB)\")\n",
    "    print(\"2. Network issues downloading the model\")\n",
    "    print(\"3. Model not yet available on HuggingFace Hub\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1754124125405,
     "user": {
      "displayName": "Ido Avnir",
      "userId": "14416193283357805409"
     },
     "user_tz": -180
    },
    "id": "th7BTIEYw4ln",
    "outputId": "28cf26cf-9e0f-4381-c400-66a5ad2e4f82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found: gr1_arms_waist.TrayToPot\n",
      "\n",
      "📊 Total available tasks: 1\n"
     ]
    }
   ],
   "source": [
    "# Discover and validate downloaded tasks\n",
    "TASKS = [\n",
    "    #\"gr1_arms_waist.CanToDrawer\",\n",
    "    #\"gr1_arms_waist.CupToDrawer\",\n",
    "    #\"gr1_arms_waist.PlaceBottleToCabinet\",\n",
    "    #\"gr1_arms_waist.PlacematToBowl\",\n",
    "    #\"gr1_arms_waist.PotatoToMicrowave\",\n",
    "    \"gr1_arms_waist.TrayToPot\"\n",
    "]\n",
    "\n",
    "# Check which tasks are available\n",
    "available_tasks = []\n",
    "for task in TASKS:\n",
    "    task_path = os.path.join(DATASET_ROOT, task)\n",
    "    if os.path.exists(task_path):\n",
    "        available_tasks.append(task)\n",
    "        print(f\"✅ Found: {task}\")\n",
    "    else:\n",
    "        print(f\"❌ Missing: {task}\")\n",
    "\n",
    "if not available_tasks:\n",
    "    print(\"❌ No tasks found! Please run the download notebook first.\")\n",
    "    raise RuntimeError(\"No dataset found\")\n",
    "\n",
    "print(f\"\\n📊 Total available tasks: {len(available_tasks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1754124129303,
     "user": {
      "displayName": "Ido Avnir",
      "userId": "14416193283357805409"
     },
     "user_tz": -180
    },
    "id": "_AyhdNj_w4lo",
    "outputId": "ec9495dd-f5da-4636-83ec-f569e809b492"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extraction functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Define extraction functions in user's specified format\n",
    "def extract_single_step_data(policy, step_data, dataset_info):\n",
    "    \"\"\"\n",
    "    Extract VLM and diffusion outputs in the user's specified format.\n",
    "\n",
    "    Returns:\n",
    "        data_dict: Dictionary with dataset, step_data, vlm_output, final_output\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Extract VLM backbone features (without action head)\n",
    "        vlm_output = policy.get_VLM_selected_layer_output(step_data)\n",
    "\n",
    "        # Extract diffusion outputs (full inference)\n",
    "        final_output = policy.get_action(step_data)\n",
    "\n",
    "        # Create the data in user's specified format\n",
    "        data_dict = {\n",
    "            'dataset': dataset_info,  # Dataset name and info\n",
    "            'step_data': step_data,   # Original input data\n",
    "            'vlm_output': vlm_output, # VLM backbone features\n",
    "            'final_output': final_output  # Diffusion action outputs\n",
    "        }\n",
    "\n",
    "        return data_dict\n",
    "\n",
    "def save_all_extraction_data(all_data_list, output_file):\n",
    "    \"\"\"Save all extracted data to a single file in user's format.\"\"\"\n",
    "\n",
    "    # Extract first value of action.right_arm from each sample\n",
    "    right_arm_first_values = []\n",
    "    for data in all_data_list:\n",
    "        if 'action.right_arm' in data['final_output'] and len(data['final_output']['action.right_arm']) > 0:\n",
    "            right_arm_first_values.append(data['final_output']['action.right_arm'][0])\n",
    "        else:\n",
    "            right_arm_first_values.append(None)  # Handle missing data\n",
    "\n",
    "    # Extract only backbone_features from vlm_output\n",
    "    backbone_features_list = []\n",
    "    for data in all_data_list:\n",
    "        if 'backbone_features' in data['vlm_output']:\n",
    "            backbone_features_list.append(data['vlm_output']['backbone_features'])\n",
    "        else:\n",
    "            backbone_features_list.append(None)  # Handle missing data\n",
    "\n",
    "    # Combine all data\n",
    "    combined_data = {\n",
    "        'dataset': [data['dataset'] for data in all_data_list],\n",
    "        'step_data': [data['step_data'] for data in all_data_list],\n",
    "        'backbone_features': backbone_features_list,  # Only backbone_features from vlm_output\n",
    "        'action_right_arm_first': right_arm_first_values,  # Only first value of action.right_arm\n",
    "        'extraction_info': {\n",
    "            'total_samples': len(all_data_list),\n",
    "            'model_path': MODEL_PATH,\n",
    "            'embodiment_tag': EMBODIMENT_TAG\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Move tensors to CPU for saving\n",
    "    # Convert backbone_features to CPU if they are tensors\n",
    "    for i in range(len(combined_data['backbone_features'])):\n",
    "        if combined_data['backbone_features'][i] is not None and torch.is_tensor(combined_data['backbone_features'][i]):\n",
    "            combined_data['backbone_features'][i] = combined_data['backbone_features'][i].cpu()\n",
    "\n",
    "    # Convert action.right_arm first values to CPU if they are tensors\n",
    "    for i in range(len(combined_data['action_right_arm_first'])):\n",
    "        if combined_data['action_right_arm_first'][i] is not None and torch.is_tensor(combined_data['action_right_arm_first'][i]):\n",
    "            combined_data['action_right_arm_first'][i] = combined_data['action_right_arm_first'][i].cpu()\n",
    "\n",
    "    # Save to file\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(combined_data, f)\n",
    "\n",
    "    print(f\"💾 Saved all data to {output_file}\")\n",
    "    print(f\"   - Total samples: {len(all_data_list)}\")\n",
    "    print(f\"   - Data keys: {list(combined_data.keys())}\")\n",
    "    print(f\"   - Saved only backbone_features from vlm_output\")\n",
    "    print(f\"   - Saved only first value of action.right_arm from each sample\")\n",
    "\n",
    "    return len(all_data_list)\n",
    "\n",
    "print(\"✅ Extraction functions defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 BATCH PROCESSING SYSTEM FOR 150K SAMPLES (PARQUET)\n",
    "# This system handles Colab disconnections by processing data in batches\n",
    "\n",
    "import json\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "# Batch processing configuration\n",
    "BATCH_SIZE = 1000  # Process 1000 samples per batch (adjust based on memory)\n",
    "TARGET_TOTAL_SAMPLES = 150000  # Total target samples\n",
    "BATCH_OUTPUT_DIR = os.path.join(OUTPUT_DIR, \"batches_parquet\")\n",
    "PROGRESS_FILE = os.path.join(OUTPUT_DIR, \"extraction_progress_parquet.json\")\n",
    "\n",
    "# Create batch output directory\n",
    "os.makedirs(BATCH_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"🎯 Target: {TARGET_TOTAL_SAMPLES:,} samples\")\n",
    "print(f\"📦 Batch size: {BATCH_SIZE:,} samples per batch\")\n",
    "print(f\"🗂️  Batch output dir: {BATCH_OUTPUT_DIR}\")\n",
    "print(f\"📋 Progress file: {PROGRESS_FILE}\")\n",
    "print(f\"💾 Format: Parquet (much more efficient!)\")\n",
    "\n",
    "def safe_tensor_to_numpy(tensor):\n",
    "    \"\"\"\n",
    "    Safely convert PyTorch tensor to numpy, handling unsupported dtypes like bfloat16\n",
    "    \"\"\"\n",
    "    if not torch.is_tensor(tensor):\n",
    "        return tensor\n",
    "    \n",
    "    # Handle unsupported dtypes\n",
    "    if tensor.dtype == torch.bfloat16:\n",
    "        tensor = tensor.float()\n",
    "    elif tensor.dtype == torch.float16:\n",
    "        tensor = tensor.float()\n",
    "    \n",
    "    return tensor.cpu().numpy()\n",
    "\n",
    "def load_progress():\n",
    "    \"\"\"Load extraction progress from file\"\"\"\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        with open(PROGRESS_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {\n",
    "        'completed_batches': [],\n",
    "        'total_extracted': 0,\n",
    "        'last_batch_id': 0,\n",
    "        'start_time': datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "def save_progress(progress):\n",
    "    \"\"\"Save extraction progress to file\"\"\"\n",
    "    progress['last_updated'] = datetime.now().isoformat()\n",
    "    with open(PROGRESS_FILE, 'w') as f:\n",
    "        json.dump(progress, f, indent=2)\n",
    "\n",
    "def get_batch_filename(batch_id):\n",
    "    \"\"\"Get standardized batch filename\"\"\"\n",
    "    return os.path.join(BATCH_OUTPUT_DIR, f\"batch_{batch_id:04d}.parquet\")\n",
    "\n",
    "def save_batch_data(batch_data, batch_id):\n",
    "    \"\"\"Save a single batch to parquet file\"\"\"\n",
    "    \n",
    "    # Prepare data for DataFrame\n",
    "    rows = []\n",
    "    \n",
    "    for idx, data in enumerate(batch_data):\n",
    "        # Extract action.right_arm first value\n",
    "        action_right_arm_first = None\n",
    "        if 'action.right_arm' in data['final_output'] and len(data['final_output']['action.right_arm']) > 0:\n",
    "            action_val = data['final_output']['action.right_arm'][0]\n",
    "            action_right_arm_first = safe_tensor_to_numpy(action_val)\n",
    "        \n",
    "        # Extract backbone_features\n",
    "        backbone_features = None\n",
    "        original_shape = None\n",
    "        if 'backbone_features' in data['vlm_output']:\n",
    "            backbone_feat = data['vlm_output']['backbone_features']\n",
    "            if torch.is_tensor(backbone_feat):\n",
    "                # Store original shape before any conversions\n",
    "                original_shape = backbone_feat.shape\n",
    "            \n",
    "            # Safely convert to numpy\n",
    "            backbone_feat = safe_tensor_to_numpy(backbone_feat)\n",
    "            \n",
    "            # Flatten the backbone features for parquet storage\n",
    "            if backbone_feat is not None:\n",
    "                if original_shape is None:\n",
    "                    original_shape = backbone_feat.shape\n",
    "                backbone_features = backbone_feat.flatten()\n",
    "        \n",
    "        # Create row for DataFrame\n",
    "        row = {\n",
    "            'sample_id': idx,\n",
    "            'global_index': data['dataset'].get('global_index', idx),\n",
    "            'task_name': data['dataset']['task_name'],\n",
    "            'sample_index': data['dataset']['sample_index'],\n",
    "            'total_samples': data['dataset']['total_samples'],\n",
    "            \n",
    "            # Store complex data as JSON strings\n",
    "            'dataset_info': json.dumps(data['dataset']),\n",
    "            'step_data': json.dumps(data['step_data'], default=str),  # default=str for non-serializable objects\n",
    "            \n",
    "            # Store actual feature data\n",
    "            'backbone_features_shape': json.dumps(list(original_shape)) if original_shape is not None else None,\n",
    "            'backbone_features': backbone_features.tolist() if backbone_features is not None else None,\n",
    "            'action_right_arm_first': action_right_arm_first.tolist() if action_right_arm_first is not None else None,\n",
    "            \n",
    "            # Batch metadata\n",
    "            'batch_id': batch_id,\n",
    "            'extraction_time': datetime.now().isoformat(),\n",
    "        }\n",
    "        \n",
    "        rows.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Save as parquet with compression\n",
    "    batch_file = get_batch_filename(batch_id)\n",
    "    df.to_parquet(batch_file, compression='snappy', index=False)\n",
    "    \n",
    "    # Save batch metadata separately\n",
    "    batch_metadata = {\n",
    "        'batch_id': batch_id,\n",
    "        'batch_size': len(batch_data),\n",
    "        'extraction_time': datetime.now().isoformat(),\n",
    "        'model_path': MODEL_PATH,\n",
    "        'embodiment_tag': EMBODIMENT_TAG,\n",
    "        'file_format': 'parquet',\n",
    "        'compression': 'snappy'\n",
    "    }\n",
    "    \n",
    "    metadata_file = batch_file.replace('.parquet', '_metadata.json')\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(batch_metadata, f, indent=2)\n",
    "    \n",
    "    return batch_file, len(batch_data)\n",
    "\n",
    "print(\"✅ Batch processing system initialized!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Extract VLM → Diffusion Probe Training Data (150K Samples)\n",
    "\n",
    "This notebook extracts training data for a probe that predicts the model's diffusion output tokens based on the VLM's intermediate representations.\n",
    "\n",
    "**🚀 NEW: Batch Processing System for 150K Samples**\n",
    "This notebook now supports extracting 150,000 samples with automatic resume capability when Colab disconnects!\n",
    "\n",
    "**📋 What this notebook does:**\n",
    "1. Loads the downloaded GR1 arms+waist dataset from Google Drive\n",
    "2. Runs inference in batches of 2,000 samples each\n",
    "3. Extracts only `backbone_features` from VLM output and first value of `action.right_arm`\n",
    "4. Saves progress automatically - can resume after disconnections\n",
    "5. Merges all batch files into final training data\n",
    "\n",
    "**🔧 Batch Processing Features:**\n",
    "- **Resume capability**: Automatically continues from where it left off\n",
    "- **Progress tracking**: JSON progress file saved to Google Drive\n",
    "- **Batch files**: Individual 2K sample batches saved separately\n",
    "- **Final merge**: Combines all batches into single training file\n",
    "- **Error handling**: Graceful handling of Colab disconnections\n",
    "\n",
    "**⚠️ Requirements:**\n",
    "- GPU runtime (Go to Runtime → Change runtime type → GPU)\n",
    "- Dataset downloaded using `download_gr1_arms_waist_dataset.ipynb`\n",
    "- ~20-40GB available storage for 150K samples\n",
    "\n",
    "**📚 How to use:**\n",
    "1. Run setup cells (1-15) once\n",
    "2. Run batch extraction cell (19) - this will process 150K samples\n",
    "3. If Colab disconnects, just re-run cell 19 to resume\n",
    "4. Run merge cell (21) when extraction is complete\n",
    "5. Test final data with cell (22)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔄 MAIN BATCH EXTRACTION LOOP\n",
    "# This processes data in batches and can resume from interruptions\n",
    "\n",
    "def extract_batches(target_samples=TARGET_TOTAL_SAMPLES, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Main function to extract data in batches with resume capability\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load existing progress\n",
    "    progress = load_progress()\n",
    "    print(f\"📊 Current progress: {progress['total_extracted']:,} samples extracted\")\n",
    "    \n",
    "    if progress['total_extracted'] >= target_samples:\n",
    "        print(f\"✅ Target already reached! {progress['total_extracted']:,} >= {target_samples:,}\")\n",
    "        return\n",
    "    \n",
    "    # Load dataset\n",
    "    task_name = available_tasks[0]  # Using first available task\n",
    "    task_path = os.path.join(DATASET_ROOT, task_name)\n",
    "    \n",
    "    print(f\"🔄 Loading dataset: {task_name}\")\n",
    "    dataset = LeRobotSingleDataset(\n",
    "        dataset_path=task_path,\n",
    "        modality_configs=modality_config,\n",
    "        video_backend=\"decord\",\n",
    "        video_backend_kwargs=None,\n",
    "        transforms=None,\n",
    "        embodiment_tag=EMBODIMENT_TAG,\n",
    "    )\n",
    "    \n",
    "    print(f\"📊 Dataset size: {len(dataset):,} samples\")\n",
    "    \n",
    "    # Calculate remaining work\n",
    "    remaining_samples = target_samples - progress['total_extracted']\n",
    "    start_index = progress['total_extracted']\n",
    "    \n",
    "    print(f\"🎯 Need to extract {remaining_samples:,} more samples\")\n",
    "    print(f\"▶️  Starting from index: {start_index:,}\")\n",
    "    \n",
    "    # Process in batches\n",
    "    current_batch_data = []\n",
    "    batch_id = progress['last_batch_id'] + 1\n",
    "    samples_processed = 0\n",
    "    \n",
    "    try:\n",
    "        for i in tqdm(range(start_index, min(start_index + remaining_samples, len(dataset))), \n",
    "                      desc=\"Extracting batches\"):\n",
    "            try:\n",
    "                # Get sample data\n",
    "                step_data = dataset[i]\n",
    "                \n",
    "                # Create dataset info\n",
    "                dataset_info = {\n",
    "                    'task_name': task_name,\n",
    "                    'sample_index': i,\n",
    "                    'total_samples': len(dataset),\n",
    "                    'global_index': progress['total_extracted'] + samples_processed\n",
    "                }\n",
    "                \n",
    "                # Extract data\n",
    "                data_dict = extract_single_step_data(policy, step_data, dataset_info)\n",
    "                current_batch_data.append(data_dict)\n",
    "                samples_processed += 1\n",
    "                \n",
    "                # Save batch when it reaches batch_size\n",
    "                if len(current_batch_data) >= batch_size:\n",
    "                    batch_file, batch_size_actual = save_batch_data(current_batch_data, batch_id)\n",
    "                    \n",
    "                    # Update progress\n",
    "                    progress['completed_batches'].append(batch_id)\n",
    "                    progress['total_extracted'] += batch_size_actual\n",
    "                    progress['last_batch_id'] = batch_id\n",
    "                    save_progress(progress)\n",
    "                    \n",
    "                    print(f\"✅ Saved batch {batch_id:04d}: {batch_size_actual:,} samples → {progress['total_extracted']:,} total\")\n",
    "                    \n",
    "                    # Clear batch data and increment batch_id\n",
    "                    current_batch_data = []\n",
    "                    batch_id += 1\n",
    "                    \n",
    "                    # Check if we've reached our target\n",
    "                    if progress['total_extracted'] >= target_samples:\n",
    "                        print(f\"🎉 Target reached! {progress['total_extracted']:,} samples extracted\")\n",
    "                        break\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  Failed to process sample {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save any remaining data in the last batch\n",
    "        if current_batch_data:\n",
    "            batch_file, batch_size_actual = save_batch_data(current_batch_data, batch_id)\n",
    "            progress['completed_batches'].append(batch_id)\n",
    "            progress['total_extracted'] += batch_size_actual\n",
    "            progress['last_batch_id'] = batch_id\n",
    "            save_progress(progress)\n",
    "            print(f\"✅ Saved final batch {batch_id:04d}: {batch_size_actual:,} samples → {progress['total_extracted']:,} total\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"\\n⏸️  Extraction interrupted by user\")\n",
    "        # Save any partial batch\n",
    "        if current_batch_data:\n",
    "            batch_file, batch_size_actual = save_batch_data(current_batch_data, batch_id)\n",
    "            progress['completed_batches'].append(batch_id)\n",
    "            progress['total_extracted'] += batch_size_actual\n",
    "            progress['last_batch_id'] = batch_id\n",
    "            save_progress(progress)\n",
    "            print(f\"💾 Saved partial batch {batch_id:04d}: {batch_size_actual:,} samples\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during extraction: {e}\")\n",
    "        # Save any partial batch\n",
    "        if current_batch_data:\n",
    "            try:\n",
    "                batch_file, batch_size_actual = save_batch_data(current_batch_data, batch_id)\n",
    "                progress['completed_batches'].append(batch_id)\n",
    "                progress['total_extracted'] += batch_size_actual\n",
    "                progress['last_batch_id'] = batch_id\n",
    "                save_progress(progress)\n",
    "                print(f\"💾 Saved partial batch {batch_id:04d}: {batch_size_actual:,} samples\")\n",
    "            except:\n",
    "                print(\"❌ Failed to save partial batch\")\n",
    "    \n",
    "    # Final summary\n",
    "    final_progress = load_progress()\n",
    "    print(f\"\\n📊 Extraction summary:\")\n",
    "    print(f\"   • Total extracted: {final_progress['total_extracted']:,} samples\")\n",
    "    print(f\"   • Batches completed: {len(final_progress['completed_batches'])}\")\n",
    "    print(f\"   • Progress: {final_progress['total_extracted']/target_samples*100:.1f}%\")\n",
    "    \n",
    "    return final_progress\n",
    "\n",
    "print(\"✅ Batch extraction function ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 INSPECT SAMPLE BATCH PARQUET FILE\n",
    "# Check the structure and columns of existing batch files\n",
    "\n",
    "def inspect_batch_files():\n",
    "    \"\"\"Inspect the structure of batch parquet files\"\"\"\n",
    "    \n",
    "    # Find batch files\n",
    "    batch_files = glob.glob(os.path.join(BATCH_OUTPUT_DIR, \"batch_*.parquet\"))\n",
    "    \n",
    "    if not batch_files:\n",
    "        print(\"❌ No batch files found to inspect!\")\n",
    "        print(f\"Looking in: {BATCH_OUTPUT_DIR}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"🔍 Found {len(batch_files)} batch files\")\n",
    "    print(f\"📁 Inspecting: {batch_files[0]}\")\n",
    "    \n",
    "    try:\n",
    "        # Read the first batch file\n",
    "        df = pd.read_parquet(batch_files[0])\n",
    "        \n",
    "        print(f\"\\n📊 File Info:\")\n",
    "        print(f\"   • Rows: {len(df)}\")\n",
    "        print(f\"   • Columns: {len(df.columns)}\")\n",
    "        print(f\"   • File size: {os.path.getsize(batch_files[0]) / (1024*1024):.1f} MB\")\n",
    "        \n",
    "        print(f\"\\n📋 Column Names:\")\n",
    "        for i, col in enumerate(df.columns):\n",
    "            print(f\"   {i+1:2d}. {col}\")\n",
    "        \n",
    "        print(f\"\\n🔍 Column Data Types:\")\n",
    "        for col in df.columns:\n",
    "            print(f\"   {col}: {df[col].dtype}\")\n",
    "        \n",
    "        print(f\"\\n📝 Sample Data (First Row):\")\n",
    "        if len(df) > 0:\n",
    "            first_row = df.iloc[0]\n",
    "            for col in df.columns:\n",
    "                val = first_row[col]\n",
    "                if pd.isna(val):\n",
    "                    print(f\"   {col}: NaN\")\n",
    "                elif isinstance(val, list):\n",
    "                    print(f\"   {col}: list with {len(val)} elements\")\n",
    "                elif isinstance(val, str):\n",
    "                    if len(val) > 100:\n",
    "                        print(f\"   {col}: string (truncated): '{val[:100]}...'\")\n",
    "                    else:\n",
    "                        print(f\"   {col}: string: '{val}'\")\n",
    "                else:\n",
    "                    print(f\"   {col}: {type(val).__name__} = {val}\")\n",
    "        \n",
    "        # Check for the specific columns we need\n",
    "        print(f\"\\n✅ Required Columns Check:\")\n",
    "        required_cols = ['backbone_features', 'backbone_features_shape', 'action_right_arm_first']\n",
    "        for col in required_cols:\n",
    "            if col in df.columns:\n",
    "                non_null_count = df[col].notna().sum()\n",
    "                print(f\"   ✅ {col}: Present ({non_null_count}/{len(df)} non-null)\")\n",
    "            else:\n",
    "                print(f\"   ❌ {col}: Missing\")\n",
    "        \n",
    "        # Show statistics for backbone_features if it exists\n",
    "        if 'backbone_features' in df.columns:\n",
    "            print(f\"\\n📊 backbone_features Analysis:\")\n",
    "            backbone_col = df['backbone_features']\n",
    "            non_null_features = backbone_col.dropna()\n",
    "            if len(non_null_features) > 0:\n",
    "                sample_features = non_null_features.iloc[0]\n",
    "                if isinstance(sample_features, list):\n",
    "                    print(f\"   • Sample length: {len(sample_features)} elements\")\n",
    "                    print(f\"   • Data type of elements: {type(sample_features[0]).__name__ if sample_features else 'empty'}\")\n",
    "                    \n",
    "        # Show statistics for shapes if it exists\n",
    "        if 'backbone_features_shape' in df.columns:\n",
    "            print(f\"\\n📊 backbone_features_shape Analysis:\")\n",
    "            shape_col = df['backbone_features_shape']\n",
    "            non_null_shapes = shape_col.dropna()\n",
    "            if len(non_null_shapes) > 0:\n",
    "                sample_shape = non_null_shapes.iloc[0]\n",
    "                if isinstance(sample_shape, str):\n",
    "                    try:\n",
    "                        parsed_shape = json.loads(sample_shape)\n",
    "                        print(f\"   • Sample shape: {parsed_shape}\")\n",
    "                        print(f\"   • Expected elements: {np.prod(parsed_shape):,}\")\n",
    "                    except:\n",
    "                        print(f\"   • Raw shape string: {sample_shape}\")\n",
    "                elif isinstance(sample_shape, list):\n",
    "                    print(f\"   • Sample shape: {sample_shape}\")\n",
    "                    print(f\"   • Expected elements: {np.prod(sample_shape):,}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error reading batch file: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run the inspection\n",
    "print(\"🔍 Inspecting batch parquet files...\")\n",
    "inspect_batch_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔗 SELECTIVE MERGE WITH MEAN POOLING AND LAST VECTOR EXTRACTION\n",
    "# This creates a compact file with processed VLM features only (NO full original merge to save space)\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm \n",
    "\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/probe_training_data\"\n",
    "BATCH_OUTPUT_DIR = os.path.join(OUTPUT_DIR, \"batches_parquet\")\n",
    "def apply_mean_pooling_and_last_vector(backbone_features, original_shape):\n",
    "    \"\"\"\n",
    "    Apply mean pooling and extract last vector from backbone features.\n",
    "    \n",
    "    Args:\n",
    "        backbone_features: Flattened list/array of backbone features\n",
    "        original_shape: Original tensor shape [batch, seq_len, hidden_size]\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (mean_pooled_features, last_vector_features) both shape [hidden_size]\n",
    "    \"\"\"\n",
    "    if backbone_features is None or original_shape is None:\n",
    "        return None, None\n",
    "    \n",
    "    if len(backbone_features) == 0 or len(original_shape) == 0:\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        # Convert to numpy array\n",
    "        features_array = np.array(backbone_features)\n",
    "        \n",
    "        # Validate that we have enough elements for the reshape\n",
    "        expected_elements = np.prod(original_shape)\n",
    "        if features_array.size != expected_elements:\n",
    "            print(f\"⚠️  Shape mismatch: got {features_array.size} elements, expected {expected_elements} for shape {original_shape}\")\n",
    "            return None, None\n",
    "        \n",
    "        # Reconstruct original shape from flattened features\n",
    "        features_reshaped = features_array.reshape(original_shape)\n",
    "        \n",
    "        # Handle different input shapes\n",
    "        if len(features_reshaped.shape) == 3:\n",
    "            # Shape is [batch, seq_len, hidden_size]\n",
    "            if features_reshaped.shape[0] == 1:\n",
    "                # Remove batch dimension -> [seq_len, hidden_size]\n",
    "                features_reshaped = features_reshaped[0]\n",
    "            else:\n",
    "                # Multiple batch items, take the first one\n",
    "                features_reshaped = features_reshaped[0]\n",
    "                \n",
    "        elif len(features_reshaped.shape) == 2:\n",
    "            # Already in [seq_len, hidden_size] format\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"⚠️  Unexpected shape after reshape: {features_reshaped.shape}\")\n",
    "            return None, None\n",
    "        \n",
    "        # Ensure we have a 2D tensor [seq_len, hidden_size]\n",
    "        if len(features_reshaped.shape) != 2:\n",
    "            print(f\"⚠️  Expected 2D features after processing, got {features_reshaped.shape}\")\n",
    "            return None, None\n",
    "        \n",
    "        seq_len, hidden_size = features_reshaped.shape\n",
    "        \n",
    "        if seq_len == 0 or hidden_size == 0:\n",
    "            print(f\"⚠️  Invalid dimensions: seq_len={seq_len}, hidden_size={hidden_size}\")\n",
    "            return None, None\n",
    "        \n",
    "        # Apply mean pooling across sequence dimension (axis 0)\n",
    "        mean_pooled = np.mean(features_reshaped, axis=0)  # [seq_len, hidden_size] -> [hidden_size]\n",
    "        \n",
    "        # Extract last vector (final sequence position)\n",
    "        last_vector = features_reshaped[-1]  # [seq_len, hidden_size] -> [hidden_size]\n",
    "        \n",
    "        # Validate output shapes\n",
    "        if mean_pooled.shape != (hidden_size,) or last_vector.shape != (hidden_size,):\n",
    "            print(f\"⚠️  Output shape mismatch: mean_pooled={mean_pooled.shape}, last_vector={last_vector.shape}, expected=({hidden_size},)\")\n",
    "            return None, None\n",
    "        \n",
    "        return mean_pooled, last_vector\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Error processing features with shape {original_shape}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def merge_batches_with_pooled_features(output_filename=\"probe_training_data_150k_processed.parquet\"):\n",
    "    \"\"\"\n",
    "    Merge batch files into a compact file with mean-pooled and last vector features.\n",
    "    This saves ~300x space compared to storing full original features.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"getting files from {BATCH_OUTPUT_DIR}/batch_*.parquet\")\n",
    "    \n",
    "    # Find all batch files\n",
    "    batch_files = glob.glob(os.path.join(BATCH_OUTPUT_DIR, \"batch_*.parquet\"))\n",
    "    batch_files.sort()  # Sort to process in order\n",
    "    \n",
    "    if not batch_files:\n",
    "        print(\"❌ No batch files found to merge!\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"🔗 Found {len(batch_files)} parquet batch files for selective merge\")\n",
    "    print(f\"🎯 Creating compact file with mean-pooled + last vector features\")\n",
    "    \n",
    "    # Process each batch file and extract compact features\n",
    "    processed_rows = []\n",
    "    total_samples = 0\n",
    "    skipped_samples = 0\n",
    "\n",
    "    for batch_file in batch_files:\n",
    "        try:\n",
    "            # Read batch parquet file\n",
    "            df = pd.read_parquet(batch_file)\n",
    "            print(f\"📁 Processing {os.path.basename(batch_file)}: {len(df)} rows\")\n",
    "            \n",
    "            # Process each sample in the batch\n",
    "            for idx, row in df.iterrows():\n",
    "                # Only process rows that have valid backbone features\n",
    "                backbone_features = row.get('backbone_features')\n",
    "                \n",
    "                if backbone_features is None or len(backbone_features) == 0:\n",
    "                    print(f\"⚠️  Skipping row {idx} in {os.path.basename(batch_file)}: No backbone features\")\n",
    "                    skipped_samples += 1\n",
    "                    continue\n",
    "                \n",
    "                # Parse original shape\n",
    "                shape_str = row.get('backbone_features_shape')\n",
    "                if shape_str:\n",
    "                    try:\n",
    "                        original_shape = json.loads(shape_str)\n",
    "                    except (json.JSONDecodeError, TypeError):\n",
    "                        print(f\"⚠️  Skipping row {idx}: Invalid shape format\")\n",
    "                        skipped_samples += 1\n",
    "                        continue\n",
    "                else:\n",
    "                    print(f\"⚠️  Skipping row {idx}: No shape information\")\n",
    "                    skipped_samples += 1\n",
    "                    continue\n",
    "                \n",
    "                # Apply mean pooling and extract last vector\n",
    "                mean_pooled, last_vector = apply_mean_pooling_and_last_vector(\n",
    "                    backbone_features, original_shape\n",
    "                )\n",
    "                \n",
    "                # Only include rows where both mean_pooled and last_vector are successfully computed\n",
    "                if mean_pooled is not None and last_vector is not None:\n",
    "                    # Create compact row with required data\n",
    "                    processed_row = {\n",
    "                        'sample_index': row.get('sample_index'),\n",
    "                        'batch_id': row.get('batch_id'),\n",
    "                        'task_name': row.get('task_name'),\n",
    "                        'backbone_features_mean_pooled': mean_pooled.tolist(),\n",
    "                        'backbone_features_last_vector': last_vector.tolist(),\n",
    "                        'action_right_arm_first': row.get('action_right_arm_first'),\n",
    "                        'original_shape': original_shape,  # Keep shape for reference\n",
    "                    }\n",
    "                    \n",
    "                    processed_rows.append(processed_row)\n",
    "                    total_samples += 1\n",
    "                else:\n",
    "                    print(f\"⚠️  Skipping row {idx}: Failed to compute pooled features\")\n",
    "                    skipped_samples += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Error processing {batch_file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if total_samples == 0:\n",
    "        print(\"❌ No valid samples found in batch files!\")\n",
    "        return None\n",
    "    \n",
    "    # Create final DataFrame with processed features\n",
    "    print(f\"🔄 Creating final DataFrame with {total_samples:,} processed samples...\")\n",
    "    print(f\"⚠️  Skipped {skipped_samples:,} samples due to missing/invalid data\")\n",
    "    \n",
    "    final_df = pd.DataFrame(processed_rows)\n",
    "    \n",
    "    # Save processed file\n",
    "    final_output_file = os.path.join(OUTPUT_DIR, output_filename)\n",
    "    \n",
    "    print(f\"💾 Saving processed parquet file to: {final_output_file}\")\n",
    "    final_df.to_parquet(final_output_file, compression='snappy', index=False)\n",
    "    \n",
    "    # Calculate feature dimension\n",
    "    if len(processed_rows) > 0 and processed_rows[0]['backbone_features_mean_pooled']:\n",
    "        feature_size = len(processed_rows[0]['backbone_features_mean_pooled'])\n",
    "        print(f\"🎯 Feature dimension: {feature_size} (both mean-pooled and last vector)\")\n",
    "    \n",
    "    print(f\"✅ Selective merge completed!\")\n",
    "    print(f\"   • Total samples: {total_samples:,}\")\n",
    "    print(f\"   • Skipped samples: {skipped_samples:,}\")\n",
    "    print(f\"   • Success rate: {total_samples/(total_samples+skipped_samples)*100:.1f}%\")\n",
    "    print(f\"   • Features: Mean-pooled + Last vector (both {feature_size}D)\")\n",
    "    print(f\"   • Final file: {final_output_file}\")\n",
    "    print(f\"   • File size: {os.path.getsize(final_output_file) / (1024*1024):.1f} MB\")\n",
    "    \n",
    "    return final_output_file, total_samples\n",
    "\n",
    "def check_batch_status():\n",
    "    \"\"\"Check current status of batch extraction\"\"\"\n",
    "    progress = load_progress()\n",
    "    batch_files = glob.glob(os.path.join(BATCH_OUTPUT_DIR, \"batch_*.parquet\"))\n",
    "    \n",
    "    print(f\"📊 Batch Extraction Status (Parquet):\")\n",
    "    print(f\"   • Progress file: {PROGRESS_FILE}\")\n",
    "    print(f\"   • Total extracted: {progress['total_extracted']:,} samples\")\n",
    "    print(f\"   • Completed batches: {len(progress['completed_batches'])}\")\n",
    "    print(f\"   • Last batch ID: {progress['last_batch_id']}\")\n",
    "    print(f\"   • Parquet files on disk: {len(batch_files)}\")\n",
    "    print(f\"   • Target progress: {progress['total_extracted']/TARGET_TOTAL_SAMPLES*100:.1f}%\")\n",
    "    \n",
    "    if batch_files:\n",
    "        # Calculate total size of batch files\n",
    "        total_size_mb = sum(os.path.getsize(f) for f in batch_files) / (1024*1024)\n",
    "        print(f\"   • Total batch size: {total_size_mb:.1f} MB\")\n",
    "        print(f\"   • Batch files: {sorted([os.path.basename(f) for f in batch_files[:10]])}\")  # Show first 10\n",
    "        if len(batch_files) > 10:\n",
    "            print(f\"     ... and {len(batch_files)-10} more\")\n",
    "    \n",
    "    return progress\n",
    "\n",
    "print(\"✅ Selective merge functions ready!\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Extract VLM → Diffusion Probe Training Data (150K Samples)\n",
    "\n",
    "This notebook extracts training data for a probe that predicts the model's diffusion output tokens based on the VLM's intermediate representations.\n",
    "\n",
    "**🚀 NEW: Batch Processing System for 150K Samples**\n",
    "This notebook now supports extracting 150,000 samples with automatic resume capability when Colab disconnects!\n",
    "\n",
    "**📋 What this notebook does:**\n",
    "1. Loads the downloaded GR1 arms+waist dataset from Google Drive\n",
    "2. Runs inference in batches of 2,000 samples each\n",
    "3. Extracts only `backbone_features` from VLM output and first value of `action.right_arm`\n",
    "4. Saves progress automatically - can resume after disconnections\n",
    "5. Merges all batch files into final training data\n",
    "\n",
    "**🔧 Batch Processing Features:**\n",
    "- **Resume capability**: Automatically continues from where it left off\n",
    "- **Progress tracking**: JSON progress file saved to Google Drive\n",
    "- **Batch files**: Individual 2K sample batches saved separately\n",
    "- **Final merge**: Combines all batches into single training file\n",
    "- **Error handling**: Graceful handling of Colab disconnections\n",
    "\n",
    "**⚠️ Requirements:**\n",
    "- GPU runtime (Go to Runtime → Change runtime type → GPU)\n",
    "- Dataset downloaded using `download_gr1_arms_waist_dataset.ipynb`\n",
    "- ~20-40GB available storage for 150K samples\n",
    "\n",
    "**📚 How to use:**\n",
    "1. Run setup cells (1-15) once\n",
    "2. Run batch extraction cell (19) - this will process 150K samples\n",
    "3. If Colab disconnects, just re-run cell 19 to resume\n",
    "4. Run merge cell (21) when extraction is complete\n",
    "5. Test final data with cell (22)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Extract VLM → Diffusion Probe Training Data (150K Samples)\n",
    "\n",
    "This notebook extracts training data for a probe that predicts the model's diffusion output tokens based on the VLM's intermediate representations.\n",
    "\n",
    "**🚀 NEW: Batch Processing System for 150K Samples**\n",
    "This notebook now supports extracting 150,000 samples with automatic resume capability when Colab disconnects!\n",
    "\n",
    "**📋 What this notebook does:**\n",
    "1. Loads the downloaded GR1 arms+waist dataset from Google Drive\n",
    "2. Runs inference in batches of 2,000 samples each\n",
    "3. Extracts only `backbone_features` from VLM output and first value of `action.right_arm`\n",
    "4. Saves progress automatically - can resume after disconnections\n",
    "5. Merges all batch files into final training data\n",
    "\n",
    "**🔧 Batch Processing Features:**\n",
    "- **Resume capability**: Automatically continues from where it left off\n",
    "- **Progress tracking**: JSON progress file saved to Google Drive\n",
    "- **Batch files**: Individual 2K sample batches saved separately\n",
    "- **Final merge**: Combines all batches into single training file\n",
    "- **Error handling**: Graceful handling of Colab disconnections\n",
    "\n",
    "**⚠️ Requirements:**\n",
    "- GPU runtime (Go to Runtime → Change runtime type → GPU)\n",
    "- Dataset downloaded using `download_gr1_arms_waist_dataset.ipynb`\n",
    "- ~20-40GB available storage for 150K samples\n",
    "\n",
    "**📚 How to use:**\n",
    "1. Run setup cells (1-15) once\n",
    "2. Run batch extraction cell (19) - this will process 150K samples\n",
    "3. If Colab disconnects, just re-run cell 19 to resume\n",
    "4. Run merge cell (21) when extraction is complete\n",
    "5. Test final data with cell (22)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Extract VLM → Diffusion Probe Training Data (150K Samples)\n",
    "\n",
    "This notebook extracts training data for a probe that predicts the model's diffusion output tokens based on the VLM's intermediate representations.\n",
    "\n",
    "**🚀 NEW: Batch Processing System for 150K Samples**\n",
    "This notebook now supports extracting 150,000 samples with automatic resume capability when Colab disconnects!\n",
    "\n",
    "**📋 What this notebook does:**\n",
    "1. Loads the downloaded GR1 arms+waist dataset from Google Drive\n",
    "2. Runs inference in batches of 2,000 samples each\n",
    "3. Extracts only `backbone_features` from VLM output and first value of `action.right_arm`\n",
    "4. Saves progress automatically - can resume after disconnections\n",
    "5. Merges all batch files into final training data\n",
    "\n",
    "**🔧 Batch Processing Features:**\n",
    "- **Resume capability**: Automatically continues from where it left off\n",
    "- **Progress tracking**: JSON progress file saved to Google Drive\n",
    "- **Batch files**: Individual 2K sample batches saved separately\n",
    "- **Final merge**: Combines all batches into single training file\n",
    "- **Error handling**: Graceful handling of Colab disconnections\n",
    "\n",
    "**⚠️ Requirements:**\n",
    "- GPU runtime (Go to Runtime → Change runtime type → GPU)\n",
    "- Dataset downloaded using `download_gr1_arms_waist_dataset.ipynb`\n",
    "- ~20-40GB available storage for 150K samples\n",
    "\n",
    "**📚 How to use:**\n",
    "1. Run setup cells (1-15) once\n",
    "2. Run batch extraction cell (19) - this will process 150K samples\n",
    "3. If Colab disconnects, just re-run cell 19 to resume\n",
    "4. Run merge cell (21) when extraction is complete\n",
    "5. Test final data with cell (22)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Extract VLM → Diffusion Probe Training Data (150K Samples)\n",
    "\n",
    "This notebook extracts training data for a probe that predicts the model's diffusion output tokens based on the VLM's intermediate representations.\n",
    "\n",
    "**🚀 NEW: Batch Processing System for 150K Samples**\n",
    "This notebook now supports extracting 150,000 samples with automatic resume capability when Colab disconnects!\n",
    "\n",
    "**📋 What this notebook does:**\n",
    "1. Loads the downloaded GR1 arms+waist dataset from Google Drive\n",
    "2. Runs inference in batches of 2,000 samples each\n",
    "3. Extracts only `backbone_features` from VLM output and first value of `action.right_arm`\n",
    "4. Saves progress automatically - can resume after disconnections\n",
    "5. Merges all batch files into final training data\n",
    "\n",
    "**🔧 Batch Processing Features:**\n",
    "- **Resume capability**: Automatically continues from where it left off\n",
    "- **Progress tracking**: JSON progress file saved to Google Drive\n",
    "- **Batch files**: Individual 2K sample batches saved separately\n",
    "- **Final merge**: Combines all batches into single training file\n",
    "- **Error handling**: Graceful handling of Colab disconnections\n",
    "\n",
    "**⚠️ Requirements:**\n",
    "- GPU runtime (Go to Runtime → Change runtime type → GPU)\n",
    "- Dataset downloaded using `download_gr1_arms_waist_dataset.ipynb`\n",
    "- ~20-40GB available storage for 150K samples\n",
    "\n",
    "**📚 How to use:**\n",
    "1. Run setup cells (1-15) once\n",
    "2. Run batch extraction cell (19) - this will process 150K samples\n",
    "3. If Colab disconnects, just re-run cell 19 to resume\n",
    "4. Run merge cell (21) when extraction is complete\n",
    "5. Test final data with cell (22)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 START BATCH EXTRACTION\n",
    "# Run this cell to start or resume batch extraction\n",
    "\n",
    "print(\"🚀 Starting batch extraction for 150K samples...\")\n",
    "print(\"⚠️  This will run until interrupted or completed\")\n",
    "print(\"📝 Progress is automatically saved - you can resume after Colab disconnects\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Check current status first\n",
    "check_batch_status()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"▶️  Starting extraction...\")\n",
    "\n",
    "# Start extraction (this will resume from where it left off)\n",
    "final_progress = extract_batches()\n",
    "\n",
    "print(\"\\n🎉 Batch extraction completed!\")\n",
    "print(\"🔗 You can now merge the batches or continue later\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔗 CREATE SELECTIVE MERGE WITH MEAN POOLING + LAST VECTOR\n",
    "# This creates a compact processed file WITHOUT merging all original data (saves space!)\n",
    "\n",
    "print(\"🔗 Creating selective merge with mean-pooled and last vector features...\")\n",
    "# Create selective merge with processed features\n",
    "processed_file, total_samples = merge_batches_with_pooled_features()\n",
    "\n",
    "if processed_file:\n",
    "    print(f\"\\n🎉 Processed parquet file ready!\")\n",
    "    print(f\"📁 Location: {processed_file}\")\n",
    "    print(f\"📊 Total samples: {total_samples:,}\")\n",
    "    \n",
    "    # Display file size and savings\n",
    "    file_size_mb = os.path.getsize(processed_file) / (1024*1024)\n",
    "    print(f\"💾 Processed file size: {file_size_mb:.1f} MB\")\n",
    "    \n",
    "    # Compare with original batch files size\n",
    "    batch_files = glob.glob(os.path.join(BATCH_OUTPUT_DIR, \"batch_*.parquet\"))\n",
    "    if batch_files:\n",
    "        original_size_mb = sum(os.path.getsize(f) for f in batch_files) / (1024*1024)\n",
    "        savings = (1 - file_size_mb/original_size_mb) * 100\n",
    "        print(f\"📦 Original batches size: {original_size_mb:.1f} MB\")\n",
    "        print(f\"🗜️  Space savings: {savings:.1f}% smaller\")\n",
    "    \n",
    "    print(f\"\\n📋 Final output summary:\")\n",
    "    print(f\"   • Original batch files: Kept for detailed analysis\")\n",
    "    print(f\"   • Processed file: Mean-pooled + last vector features\")\n",
    "    print(f\"   • Feature dimensions: [2048] for both mean and last vector\")\n",
    "    print(f\"   • Ready for probe training!\")\n",
    "else:\n",
    "    print(\"❌ Selective merge failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧪 TEST PROCESSED DATA LOADING AND VALIDATION\n",
    "# Run this cell to verify the processed file works correctly\n",
    "\n",
    "def test_processed_data():\n",
    "    \"\"\"Test loading and validating the processed parquet file\"\"\"\n",
    "    \n",
    "    # Find the most recent processed file\n",
    "    processed_files = glob.glob(os.path.join(OUTPUT_DIR, \"*_processed.parquet\"))\n",
    "    \n",
    "    if not processed_files:\n",
    "        print(\"❌ No processed files found. Run the merge cell first.\")\n",
    "        return\n",
    "    \n",
    "    # Use most recent file\n",
    "    processed_file = sorted(processed_files)[-1]\n",
    "    print(f\"🔍 Testing processed file: {os.path.basename(processed_file)}\")\n",
    "    \n",
    "    try:\n",
    "        # Load processed data\n",
    "        df = pd.read_parquet(processed_file)\n",
    "        \n",
    "        print(f\"✅ Successfully loaded processed data!\")\n",
    "        print(f\"   • Total samples: {len(df):,}\")\n",
    "        print(f\"   • Columns: {len(df.columns)}\")\n",
    "        print(f\"   • File size: {os.path.getsize(processed_file) / (1024*1024):.1f} MB\")\n",
    "        \n",
    "        # Check feature dimensions\n",
    "        sample_row = df.iloc[0]\n",
    "        \n",
    "        if sample_row.get('backbone_features_mean_pooled') is not None:\n",
    "            mean_pooled = sample_row['backbone_features_mean_pooled']\n",
    "            mean_dim = len(mean_pooled) if isinstance(mean_pooled, list) else \"Unknown\"\n",
    "            print(f\"   • Mean-pooled features: {mean_dim}D\")\n",
    "        else:\n",
    "            print(\"   ⚠️  No mean-pooled features found\")\n",
    "            \n",
    "        if sample_row.get('backbone_features_last_vector') is not None:\n",
    "            last_vector = sample_row['backbone_features_last_vector']\n",
    "            last_dim = len(last_vector) if isinstance(last_vector, list) else \"Unknown\"\n",
    "            print(f\"   • Last vector features: {last_dim}D\")\n",
    "        else:\n",
    "            print(\"   ⚠️  No last vector features found\")\n",
    "        \n",
    "        # Check action data\n",
    "        action_data = sample_row.get('action_right_arm_first')\n",
    "        if action_data is not None:\n",
    "            action_dim = len(action_data) if isinstance(action_data, list) else \"Unknown\"\n",
    "            print(f\"   • Action dimensions: {action_dim}D\")\n",
    "        \n",
    "        # Validate data integrity\n",
    "        null_mean = df['backbone_features_mean_pooled'].isnull().sum()\n",
    "        null_last = df['backbone_features_last_vector'].isnull().sum()\n",
    "        null_action = df['action_right_arm_first'].isnull().sum()\n",
    "        \n",
    "        print(f\"\\n🔍 Data integrity check:\")\n",
    "        print(f\"   • Null mean-pooled features: {null_mean}/{len(df)} ({null_mean/len(df)*100:.1f}%)\")\n",
    "        print(f\"   • Null last vector features: {null_last}/{len(df)} ({null_last/len(df)*100:.1f}%)\")\n",
    "        print(f\"   • Null action data: {null_action}/{len(df)} ({null_action/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        if null_mean < len(df) * 0.1 and null_last < len(df) * 0.1:\n",
    "            print(\"✅ Data integrity looks good!\")\n",
    "        else:\n",
    "            print(\"⚠️  High number of null values detected\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error testing processed data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run the test\n",
    "print(\"🧪 Testing processed data file...\")\n",
    "test_df = test_processed_data()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03c12a24d6b24679b91ba06daa055b87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4933e9ff76af46f0a40b01ec4b5c7ba9",
      "placeholder": "​",
      "style": "IPY_MODEL_e2da10c5949544ab80b4a1fcc7cd4590",
      "value": "Fetching 13 files: 100%"
     }
    },
    "13c4394cb3974c11ae99d5ebf68c9e58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_695b9654847b44e4b72302bd1f554fc6",
      "placeholder": "​",
      "style": "IPY_MODEL_8c679d1a038a49fa954ed466c9db66ee",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "20f981da443343ef8938f1a5e7da1f73": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_13c4394cb3974c11ae99d5ebf68c9e58",
       "IPY_MODEL_bdf31c1f0f424bfd862580ef1171587e",
       "IPY_MODEL_878eb4472ff24b4ba1d63f9b8256fe38"
      ],
      "layout": "IPY_MODEL_e4efdc12494f47a7aaf62447125bd21a"
     }
    },
    "32547896b6a14a798b7a283df308f0ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4933e9ff76af46f0a40b01ec4b5c7ba9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a20538703a94164b32d76daff689c1e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5503017b9d7b4a47b47b89813bdc098b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "695b9654847b44e4b72302bd1f554fc6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "729c2a98380b4c9ab9771e1c514dac3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8398b050e4044579963474d94d198a02": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "878eb4472ff24b4ba1d63f9b8256fe38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8398b050e4044579963474d94d198a02",
      "placeholder": "​",
      "style": "IPY_MODEL_a6868c18061d444cbb2b9e74abca20d3",
      "value": " 3/3 [00:00&lt;00:00,  9.50it/s]"
     }
    },
    "8c679d1a038a49fa954ed466c9db66ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "91d5bb7c2b844bbba88151828716eb14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "94afda7a63eb4ae78591298ccfdd1abe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99023573a4d4427fa8fe3b94fdb792b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a6868c18061d444cbb2b9e74abca20d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ac5551f516034786b9583800a941e681": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_03c12a24d6b24679b91ba06daa055b87",
       "IPY_MODEL_b95adae545394dd2a0fc493dea028828",
       "IPY_MODEL_f6d5e8052f18408080cac7b7b3e84f9c"
      ],
      "layout": "IPY_MODEL_4a20538703a94164b32d76daff689c1e"
     }
    },
    "b95adae545394dd2a0fc493dea028828": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_32547896b6a14a798b7a283df308f0ff",
      "max": 13,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_91d5bb7c2b844bbba88151828716eb14",
      "value": 13
     }
    },
    "bdf31c1f0f424bfd862580ef1171587e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_94afda7a63eb4ae78591298ccfdd1abe",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_729c2a98380b4c9ab9771e1c514dac3e",
      "value": 3
     }
    },
    "e2da10c5949544ab80b4a1fcc7cd4590": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e4efdc12494f47a7aaf62447125bd21a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f6d5e8052f18408080cac7b7b3e84f9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5503017b9d7b4a47b47b89813bdc098b",
      "placeholder": "​",
      "style": "IPY_MODEL_99023573a4d4427fa8fe3b94fdb792b7",
      "value": " 13/13 [00:00&lt;00:00, 1402.16it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
