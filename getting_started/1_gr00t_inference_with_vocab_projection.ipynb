{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# GR00T Inference with Simple Vocabulary Projection\n",
        "\n",
        "This notebook shows basic inference and demonstrates what \"words\" the VLM is thinking about using vocabulary projection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import gr00t\n",
        "from gr00t.data.dataset import LeRobotSingleDataset\n",
        "from gr00t.model.policy import Gr00tPolicy\n",
        "from gr00t.experiment.data_config import DATA_CONFIG_MAP\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Setup Model and Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup paths\n",
        "MODEL_PATH = \"nvidia/GR00T-N1.5-3B\"\n",
        "REPO_PATH = os.path.dirname(os.path.dirname(gr00t.__file__))\n",
        "DATASET_PATH = os.path.join(REPO_PATH, \"demo_data/robot_sim.PickNPlace\")\n",
        "EMBODIMENT_TAG = \"gr1\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load policy\n",
        "data_config = DATA_CONFIG_MAP[\"fourier_gr1_arms_only\"]\n",
        "modality_config = data_config.modality_config()\n",
        "modality_transform = data_config.transform()\n",
        "\n",
        "policy = Gr00tPolicy(\n",
        "    model_path=MODEL_PATH,\n",
        "    embodiment_tag=EMBODIMENT_TAG,\n",
        "    modality_config=modality_config,\n",
        "    modality_transform=modality_transform,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "print(\"Policy loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "dataset = LeRobotSingleDataset(\n",
        "    dataset_path=DATASET_PATH,\n",
        "    modality_configs=modality_config,\n",
        "    video_backend=\"decord\",\n",
        "    video_backend_kwargs=None,\n",
        "    transforms=None,\n",
        "    embodiment_tag=EMBODIMENT_TAG,\n",
        ")\n",
        "\n",
        "print(\"Dataset loaded successfully!\")\n",
        "\n",
        "# Get a data point\n",
        "step_data = dataset[0]\n",
        "print(f\"Task description: {step_data.get('annotation.human.action.task_description', 'No description')}\")\n",
        "\n",
        "# Show the robot's view\n",
        "if 'video.ego_view' in step_data:\n",
        "    image = step_data['video.ego_view'][0]\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(image)\n",
        "    plt.title(\"Robot's view\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run normal inference\n",
        "print(\"Running inference...\")\n",
        "predicted_action = policy.get_action(step_data)\n",
        "\n",
        "print(\"\\nPredicted actions:\")\n",
        "for key, value in predicted_action.items():\n",
        "    print(f\"  {key}: {value.shape}\")\n",
        "\n",
        "# Show joint trajectories for right arm\n",
        "right_arm_pred = predicted_action[\"action.right_arm\"]\n",
        "print(f\"\\nRight arm prediction shape: {right_arm_pred.shape}\")\n",
        "print(\"First few predictions for joint 0:\", right_arm_pred[:5, 0])\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Vocabulary Projection - What is the VLM \"thinking\"?\n",
        "\n",
        "Now let's peek inside the VLM to see what words/concepts it associates with the visual scene and task instruction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_vlm_hidden_states(policy, observations):\n",
        "    \"\"\"Extract hidden states from the VLM backbone\"\"\"\n",
        "    backbone = policy.model.backbone\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Prepare input (same as normal forward pass)\n",
        "        backbone_input = backbone.prepare_inputs(observations)\n",
        "        \n",
        "        # Get eagle model inputs\n",
        "        eagle_prefix = \"eagle_\"\n",
        "        eagle_input = {\n",
        "            k.removeprefix(eagle_prefix): v\n",
        "            for k, v in backbone_input.items()\n",
        "            if k.startswith(eagle_prefix)\n",
        "        }\n",
        "        if \"image_sizes\" in eagle_input:\n",
        "            del eagle_input[\"image_sizes\"]\n",
        "        \n",
        "        # Get hidden states from Eagle VLM\n",
        "        eagle_output = backbone.eagle_model(**eagle_input, output_hidden_states=True, return_dict=True)\n",
        "        hidden_states = eagle_output.hidden_states[backbone.select_layer]\n",
        "        \n",
        "        return hidden_states\n",
        "\n",
        "# Test getting hidden states\n",
        "print(\"Getting VLM hidden states...\")\n",
        "vlm_hidden = get_vlm_hidden_states(policy, step_data)\n",
        "print(f\"Hidden states shape: {vlm_hidden.shape}\")\n",
        "print(f\"This represents {vlm_hidden.shape[1]} token positions with {vlm_hidden.shape[2]} dimensions each\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simple_vocab_projection(hidden_states, policy, top_k=3):\n",
        "    \"\"\"Simple vocabulary projection - project hidden states back to vocabulary space\"\"\"\n",
        "    \n",
        "    # Get tokenizer\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B\")\n",
        "        print(\"Using Qwen tokenizer\")\n",
        "    except:\n",
        "        print(\"Could not load Qwen tokenizer, using GPT2\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "    \n",
        "    # Access the VLM model\n",
        "    vlm_model = policy.model.backbone.eagle_model\n",
        "    \n",
        "    # Try to find the language model head\n",
        "    if hasattr(vlm_model, 'language_model') and hasattr(vlm_model.language_model, 'lm_head'):\n",
        "        lm_head = vlm_model.language_model.lm_head\n",
        "        print(\"Found language model head in vlm_model.language_model.lm_head\")\n",
        "    elif hasattr(vlm_model, 'lm_head'):\n",
        "        lm_head = vlm_model.lm_head\n",
        "        print(\"Found language model head in vlm_model.lm_head\")\n",
        "    else:\n",
        "        print(\"Could not find language model head for vocab projection\")\n",
        "        print(f\"Available attributes: {list(vlm_model.__dict__.keys())}\")\n",
        "        return None\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Handle batch dimension\n",
        "        if hidden_states.dim() == 3:  # [batch, seq_len, hidden_dim]\n",
        "            hidden_states = hidden_states[0]  # Take first batch\n",
        "        \n",
        "        print(f\"Projecting hidden states of shape {hidden_states.shape} to vocabulary space...\")\n",
        "        \n",
        "        # Project to vocabulary space\n",
        "        logits = lm_head(hidden_states)  # [seq_len, vocab_size]\n",
        "        vocab_probs = torch.softmax(logits, dim=-1)\n",
        "        \n",
        "        print(f\"Vocabulary projection shape: {vocab_probs.shape}\")\n",
        "        \n",
        "        # Get top-k words for each position\n",
        "        seq_len = vocab_probs.shape[0]\n",
        "        results = []\n",
        "        \n",
        "        for pos in range(min(seq_len, 15)):  # Show first 15 positions\n",
        "            top_values, top_indices = torch.topk(vocab_probs[pos], top_k)\n",
        "            top_words = []\n",
        "            \n",
        "            for idx in top_indices:\n",
        "                try:\n",
        "                    word = tokenizer.decode([idx.item()]).strip()\n",
        "                    if word and len(word) > 0 and word not in ['<', '>', '|', ' ']:\n",
        "                        top_words.append(word)\n",
        "                except:\n",
        "                    pass\n",
        "            \n",
        "            if len(top_words) > 0:\n",
        "                results.append({\n",
        "                    'position': pos,\n",
        "                    'words': top_words[:top_k],\n",
        "                    'probabilities': top_values.cpu().numpy()[:len(top_words)]\n",
        "                })\n",
        "        \n",
        "        return results\n",
        "\n",
        "# Try vocabulary projection\n",
        "print(\"\\nAttempting vocabulary projection...\")\n",
        "vocab_results = simple_vocab_projection(vlm_hidden, policy, top_k=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display vocabulary projection results\n",
        "if vocab_results:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TOP WORDS AT EACH POSITION (what the VLM is 'thinking'):\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    for result in vocab_results:\n",
        "        pos = result['position']\n",
        "        words = result['words']\n",
        "        probs = result['probabilities']\n",
        "        \n",
        "        word_prob_pairs = [f\"{word}({prob:.3f})\" for word, prob in zip(words, probs)]\n",
        "        print(f\"Position {pos:2d}: {', '.join(word_prob_pairs)}\")\n",
        "        \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"INTERPRETATION GUIDE:\")\n",
        "    print(\"- Early positions (0-10): Often represent visual concepts from the image\")\n",
        "    print(\"- Later positions (10+): Often represent language concepts from instruction\")\n",
        "    print(\"- High probabilities (>0.1): Strong semantic activations\")\n",
        "    print(\"- Related words together: Semantic clusters\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "else:\n",
        "    print(\"Could not perform vocabulary projection\")\n",
        "    print(\"This might be normal - some model architectures don't expose the language model head\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
